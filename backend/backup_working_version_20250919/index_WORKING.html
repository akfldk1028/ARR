<!DOCTYPE html>
<html>
<head>
    <title>{{ page_title }}</title>
    <meta charset="UTF-8">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .container {
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
        }
        .header h1 {
            color: #333;
            margin-bottom: 10px;
        }
        .header p {
            color: #666;
            font-size: 14px;
        }
        .chat {
            border: 2px solid #e0e0e0;
            height: 500px;
            overflow-y: auto;
            padding: 20px;
            margin: 20px 0;
            background: #f9f9f9;
            border-radius: 15px;
        }
        .message {
            margin: 15px 0;
            padding: 15px;
            border-radius: 10px;
            max-width: 80%;
            animation: fadeIn 0.3s;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .user {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin-left: auto;
            text-align: right;
        }
        .ai {
            background: white;
            border: 1px solid #e0e0e0;
            margin-right: auto;
        }
        .controls {
            margin: 20px 0;
            padding: 20px;
            background: #f5f5f5;
            border-radius: 15px;
        }
        .control-group {
            margin-bottom: 20px;
        }
        .control-group h3 {
            color: #333;
            margin-bottom: 10px;
        }
        input, button {
            padding: 12px 20px;
            margin: 5px;
            border: 2px solid #e0e0e0;
            border-radius: 8px;
            font-size: 14px;
            transition: all 0.3s;
        }
        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            cursor: pointer;
            font-weight: bold;
        }
        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }
        #messageInput { width: 60%; }
        #imagePrompt { width: 50%; }
        .status {
            padding: 10px 20px;
            margin: 10px 0;
            border-radius: 8px;
            text-align: center;
            font-weight: bold;
            transition: all 0.3s;
        }
        .connected {
            background: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }
        .disconnected {
            background: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }
        .image-preview {
            max-width: 300px;
            margin: 10px 0;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        .info-badge {
            display: inline-block;
            padding: 4px 8px;
            background: #f0f0f0;
            border-radius: 4px;
            font-size: 12px;
            color: #666;
            margin-left: 10px;
        }
        .capabilities {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin-top: 10px;
        }
        .capability {
            padding: 5px 15px;
            background: #e3f2fd;
            color: #1976d2;
            border-radius: 20px;
            font-size: 12px;
            font-weight: bold;
        }

        /* Continuous Voice Audio Styles */
        .voice-controls {
            background: #f8f9fa;
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            border: 2px solid #e0e0e0;
        }
        .voice-status {
            text-align: center;
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 20px;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        .voice-status.inactive {
            background: #f0f0f0;
            color: #666;
        }
        .voice-status.active {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            animation: pulse 2s infinite;
        }
        .visualizer {
            height: 80px;
            background: #000;
            border-radius: 10px;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            overflow: hidden;
        }
        .visualizer canvas {
            width: 100%;
            height: 100%;
        }
        .voice-button-group {
            display: flex;
            gap: 10px;
            justify-content: center;
        }
        .voice-button {
            flex: 1;
            max-width: 200px;
            padding: 12px 20px;
            border: none;
            border-radius: 8px;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        .start-voice-btn {
            background: #4CAF50;
            color: white;
        }
        .start-voice-btn:hover {
            background: #45a049;
            transform: translateY(-2px);
        }
        .stop-voice-btn {
            background: #f44336;
            color: white;
        }
        .stop-voice-btn:hover {
            background: #da190b;
            transform: translateY(-2px);
        }
        .voice-button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{{ page_title }}</h1>
            <p>Model: {{ model_name }} | Django + Channels WebSocket</p>
            <div class="capabilities">
                <span class="capability">Text</span>
                <span class="capability">Image</span>
                <span class="capability">Video (Soon)</span>
                <span class="capability">Audio (Input)</span>
            </div>
        </div>

        <div id="status" class="status disconnected">Connecting to Live API...</div>

        <div id="chat" class="chat"></div>

        <div class="controls">
            <div class="control-group">
                <h3>Image Analysis</h3>
                <input type="file" id="imageInput" accept="image/*">
                <input type="text" id="imagePrompt" placeholder="Ask about the image..." value="What do you see in this image?">
                <button onclick="uploadImage()">Analyze Image</button>
            </div>

            <div class="control-group">
                <h3>ü§ñ A2A Agent Selection</h3>
                <select id="agentSelect" style="margin: 5px; width: 60%;">
                    <option value="general-worker">General Assistant (Default)</option>
                    <option value="flight-specialist">Flight Specialist</option>
                    <option value="hotel-specialist">Hotel Specialist</option>
                    <option value="travel-assistant">Travel Assistant</option>
                </select>
                <button onclick="switchAgent()">Switch Agent</button>
                <button onclick="requestAgentList()">Refresh Agents</button>
                <div id="agentStatus" style="margin: 10px 0; font-size: 12px; color: #666;">Current: General Assistant</div>
                <div id="agentInfo" style="margin: 10px 0; font-size: 11px; color: #888; font-style: italic;">Voice: Warm and helpful general assistant</div>
            </div>

            <div class="control-group">
                <h3>Audio Chat (TTS)</h3>
                <input type="text" id="audioInput" placeholder="Type message for voice response..." style="width: 50%;">
                <select id="voiceSelect" style="margin: 5px;">
                    <option value="Aoede">Aoede (Default)</option>
                    <option value="Charon">Charon</option>
                    <option value="Fenrir">Fenrir</option>
                    <option value="Kore">Kore</option>
                    <option value="Puck">Puck</option>
                </select>
                <button onclick="sendAudioMessage()">üîä Get Voice Response</button>
                <div id="audioStatus" style="margin: 10px 0; font-size: 12px; color: #666;"></div>
                <audio id="audioPlayer" controls style="display: none; margin: 10px 0;"></audio>
            </div>

            <div class="control-group">
                <h3>üéôÔ∏è Voice Conversation</h3>
                
                <!-- Audio Visualizer (from continuous_voice.html) -->
                <div style="height: 80px; background: #000; border-radius: 10px; margin: 15px 0; display: flex; align-items: center; justify-content: center; position: relative; overflow: hidden;">
                    <canvas id="visualizer" style="width: 100%; height: 100%;"></canvas>
                </div>
                
                <button id="audioButton" onclick="startContinuousConversation()">üé§ Start Real-time Conversation</button>
                <button id="stopButton" onclick="stopConversation()" style="margin-left: 10px; display: none; background: #dc3545;">‚èπÔ∏è Stop</button>
                <div id="recordingStatus" style="margin: 10px 0; font-size: 12px; color: #666;">Click to start continuous voice conversation</div>
                <div style="font-size: 11px; color: #888; margin-top: 5px;">
                    üí° One-click continuous conversation: Just speak naturally, AI responds automatically
                </div>
            </div>

            <div class="control-group">
                <h3>Text Chat</h3>
                <input type="text" id="messageInput" placeholder="Type your message..." onkeypress="if(event.key==='Enter') sendMessage()">
                <button onclick="sendMessage()">Send</button>
                <button onclick="clearChat()">Clear</button>
            </div>
        </div>
    </div>

    <script>
        console.log('Starting WebSocket connection to:', '{{ websocket_url }}');
        const ws = new WebSocket('{{ websocket_url }}');
        const chat = document.getElementById('chat');
        const status = document.getElementById('status');

        // Continuous Voice Variables (from continuous_voice.html)
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let isVoiceActive = false;
        let inputProcessor = null;
        let outputProcessor = null;

        // Official Context7 Cookbook Pattern - AudioWorklet for real-time streaming
        const inputWorkletCode = `
        class InputProcessor extends AudioWorkletProcessor {
            constructor() {
                super();
                this._out = [];
                this._out_len = 0;
                console.log("Official Context7 InputProcessor initialized", this);

                this.port.postMessage({
                    debug: "Input processor ready with official Context7 pattern!",
                });
            }

            encodeAudio(input) {
                const channel = input[0];
                const data = new ArrayBuffer(2 * channel.length);
                const view = new DataView(data);
                for (let i = 0; i < channel.length; i++) {
                    view.setInt16(2*i, channel[i] * 32767, true);
                }
                return data;
            }

            process(inputs, outputs, parameters) {
                // Official Context7 Pattern: 50ms batch transmission
                if (inputs[0] && inputs[0][0]) {
                    let data = this.encodeAudio(inputs[0]);
                    this._out.push(data);
                    this._out_len += data.byteLength;

                    // Official Gemini Cookbook: sampleRate/20 = 50ms batches
                    // Note: Input is 16kHz for microphone, but use actual sampleRate
                    if (this._out_len > (2 * 16000 / 20)) {
                        let concat = new Uint8Array(this._out_len);
                        let idx = 0;
                        for (let a of this._out) {
                            concat.set(new Uint8Array(a), idx);
                            idx += a.byteLength;
                        }
                        this._out = [];
                        this._out_len = 0;
                        this.port.postMessage({
                            'audio_in': concat.buffer,
                        });
                    }
                }
                return true;
            }
        }

        registerProcessor('input-processor', InputProcessor);
        `;

        // Context7 Cookbook OUTPUT PROCESSOR - seamless playback
        const outputWorkletCode = `
        class OutputProcessor extends AudioWorkletProcessor {
            constructor() {
                super();
                this._queue = [];
                this.port.onmessage = (event) => {
                    if ('enqueue' in event.data) {
                        this.enqueueAudio(event.data.enqueue);
                    }
                    if ('clear' in event.data) {
                        this.clearAudio();
                    }
                };
                console.log("Context7 OutputProcessor initialized");
                this.port.postMessage({
                    debug: "Output processor ready for seamless playback!",
                });
            }

            enqueueAudio(input) {
                // Context7 pattern: Convert PCM bytes to Float32
                let view = new DataView(input);
                let floats = [];
                for (let i = 0; i < input.byteLength; i += 2) {
                    floats.push(view.getInt16(i, true) / 32768.0);
                }
                this._queue.push(Float32Array.from(floats));
            }

            dequeueIntoBuffer(output) {
                // Context7 seamless buffer management
                let idx = 0;
                while (idx < output.length) {
                    if (this._queue.length === 0) {
                        // Fill remaining with silence to prevent clicks
                        output.fill(0, idx);
                        return;
                    }
                    let input = this._queue[0];
                    if (input.length == 0) {
                        this._queue.shift();
                        continue;
                    }
                    let n = Math.min(input.length, output.length - idx);
                    output.set(input.subarray(0, n), idx);
                    this._queue[0] = input.subarray(n);
                    idx += n;
                }
            }

            clearAudio() {
                this._queue = [];
            }

            process(inputs, outputs, parameters) {
                // Context7 OUTPUT: seamless audio playback
                if (outputs[0] && outputs[0][0]) {
                    this.dequeueIntoBuffer(outputs[0][0]);
                    // Copy to stereo channels
                    for (let i = 1; i < outputs[0].length; i++) {
                        const src = outputs[0][0];
                        const dst = outputs[0][i];
                        dst.set(src.subarray(0, dst.length));
                    }
                }
                return true;
            }
        }

        registerProcessor('output-processor', OutputProcessor);
        `;

        ws.onopen = () => {
            console.log('WebSocket connected!');
            status.textContent = 'Connected to Django + Gemini Live API + A2A Agents';
            status.className = 'status connected';

            // Request initial agent list
            setTimeout(() => {
                requestAgentList();
            }, 500);
        };

        ws.onclose = (event) => {
            console.log('WebSocket closed:', event.code, event.reason);
            status.textContent = 'Disconnected';
            status.className = 'status disconnected';
        };

        ws.onerror = (error) => {
            console.error('WebSocket error details:', error);
            status.textContent = 'Connection Error';
            status.className = 'status disconnected';
        };

        // Streaming variables
        let currentStreamingMessage = null;
        let streamingMessageElement = null;

        ws.onmessage = (event) => {
            const data = JSON.parse(event.data);
            console.log('WebSocket received:', data); // DEBUG

            if (data.type === 'connection') {
                addMessage(`Connected! Model: ${data.model}`, 'ai');
            } else if (data.type === 'streaming_chunk') {
                // Handle real-time streaming chunks (Gemini)
                if (!currentStreamingMessage) {
                    // Create new streaming message
                    currentStreamingMessage = '';
                    streamingMessageElement = document.createElement('div');
                    streamingMessageElement.className = 'message ai';
                    streamingMessageElement.innerHTML = '[STREAM] ';
                    chat.appendChild(streamingMessageElement);
                }

                if (data.chunk) {
                    currentStreamingMessage += data.chunk;
                    streamingMessageElement.innerHTML = `[STREAM] ${currentStreamingMessage}`;
                    chat.scrollTop = chat.scrollHeight;
                }

                if (data.is_final) {
                    // Finalize streaming message
                    streamingMessageElement.innerHTML = `[TXT] ${currentStreamingMessage}`;
                    currentStreamingMessage = null;
                    streamingMessageElement = null;
                }
            } else if (data.type === 'a2a_streaming_chunk') {
                // Handle real-time A2A agent streaming chunks
                if (!currentStreamingMessage) {
                    // Create new A2A streaming message
                    currentStreamingMessage = '';
                    streamingMessageElement = document.createElement('div');
                    streamingMessageElement.className = 'message ai';
                    streamingMessageElement.innerHTML = `[ü§ñ ${data.agent_slug}] `;
                    chat.appendChild(streamingMessageElement);
                }

                if (data.chunk) {
                    currentStreamingMessage += data.chunk;
                    streamingMessageElement.innerHTML = `[ü§ñ ${data.agent_slug}] ${currentStreamingMessage}`;
                    chat.scrollTop = chat.scrollHeight;
                }
            } else if (data.type === 'ready_for_next') {
                // Handle continuous mode ready signal
                if (data.continuous_mode && continuousMode && isRecording) {
                    const audioStatus = document.getElementById('recordingStatus');
                    audioStatus.textContent = 'üé§ Ready for your next question... Speak now!';

                    // Optionally show visual indicator
                    setTimeout(() => {
                        if (continuousMode && isRecording) {
                            audioStatus.textContent = 'üé§ Listening for your next question...';
                        }
                    }, 1000);
                }
            } else if (data.type === 'processing_started') {
                // Handle processing acknowledgment
                if (data.continuous_mode) {
                    const audioStatus = document.getElementById('recordingStatus');
                    audioStatus.textContent = `üé§ Processing: "${data.message}"...`;
                }
            } else if (data.type === 'response' || data.type === 'image_response') {
                const time = data.response_time ? ` <span class="info-badge">${data.response_time.toFixed(2)}s</span>` : '';
                let typeIcon = '[TXT]';
                if (data.type === 'image_response') typeIcon = '[IMG]';
                else if (data.analysis_type === 'image_regular_api') typeIcon = '[IMG]';
                else if (data.analysis_type === 'audio_live_api') typeIcon = '[AUD]';
                else if (data.analysis_type === 'video_frame_analysis') typeIcon = '[VID]';
                addMessage(`${typeIcon} ${data.message}${time}`, 'ai');
            } else if (data.type === 'audio_response') {
                const time = data.response_time ? ` <span class="info-badge">${data.response_time.toFixed(2)}s</span>` : '';
                const transcript = data.transcript || 'No response generated';
                const agentInfo = data.agent_name ? ` (${data.agent_name})` : '';
                addMessage(`[üîä] ${transcript}${time}${agentInfo}`, 'ai');

                // Play audio if available, otherwise use browser TTS (only in non-continuous voice mode)
                if (!isVoiceActive) {
                    if (data.audio && data.success) {
                        playAudioResponse(data.audio);
                    } else if (transcript && transcript !== 'No response generated') {
                        // Fallback to browser TTS
                        speakText(transcript, data.voice || 'Aoede');
                        document.getElementById('audioStatus').textContent = 'Playing with browser TTS...';
                    }
                }

                // Auto-restart recording in continuous mode after response
                if (continuousMode && isRecording) {
                    setTimeout(() => {
                        if (continuousMode) {
                            document.getElementById('recordingStatus').textContent = 'üé§ Listening for your next question...';
                            restartListening();
                        }
                    }, 1500); // Wait 1.5 seconds after audio response
                }
            } else if (data.type === 'agent_switched') {
                // Handle automatic agent switching
                updateCurrentAgent(data.new_agent, data.agent_name);
                addMessage(`[ü§ñ] Switched to ${data.agent_name}: ${data.message || 'Agent changed'}`, 'ai');
                document.getElementById('agentSelect').value = data.new_agent;
            } else if (data.type === 'agents_list') {
                // Handle agent list response
                updateAgentsList(data.agents, data.current_agent);
                addMessage(`[ü§ñ] Agent list updated. Current: ${data.current_agent}`, 'ai');
            } else if (data.type === 'agent_info') {
                // Handle agent info response
                updateAgentInfo(data);
            } else if (data.type === 'delegation_in_progress') {
                // Handle real-time delegation notification
                addMessage(`[üîÑ] ${data.message}`, 'ai');
            } else if (data.type === 'voice_session_status') {
                // Handle continuous voice session status (from continuous_voice.html)
                if (data.status === 'started') {
                    startAudioCapture();
                }
                addMessage(data.message, 'ai');
            } else if (data.type === 'audio_chunk') {
                // Handle real-time audio chunks (from continuous_voice.html)
                if (data.audio && outputProcessor) {
                    const decoded = Uint8Array.from(
                        atob(data.audio), c => c.charCodeAt(0)
                    ).buffer;
                    outputProcessor.port.postMessage({'enqueue': decoded});
                }
            } else if (data.type === 'transcript') {
                // Handle AI transcript (from continuous_voice.html)
                if (data.text) {
                    addMessage(`ü§ñ AI: ${data.text}`, 'ai');
                }
            } else if (data.type === 'user_transcript') {
                // Handle user transcript (from continuous_voice.html) 
                if (data.text) {
                    addMessage(`üë§ You: ${data.text}`, 'user');
                }
            } else if (data.type === 'error') {
                addMessage(`[ERROR] ${data.message}`, 'ai');
            }
        };

        function addMessage(msg, sender) {
            const div = document.createElement('div');
            div.className = 'message ' + sender;
            div.innerHTML = msg;
            chat.appendChild(div);
            chat.scrollTop = chat.scrollHeight;
        }

        function sendMessage() {
            const input = document.getElementById('messageInput');
            const msg = input.value.trim();
            if (msg && ws.readyState === WebSocket.OPEN) {
                addMessage('[TXT] ' + msg, 'user');
                ws.send(JSON.stringify({type: 'text', message: msg}));
                input.value = '';
            } else if (ws.readyState !== WebSocket.OPEN) {
                alert('Not connected. Please refresh the page.');
            }
        }

        function uploadImage() {
            const file = document.getElementById('imageInput').files[0];
            const prompt = document.getElementById('imagePrompt').value || 'What do you see in this image?';

            if (file && ws.readyState === WebSocket.OPEN) {
                const reader = new FileReader();
                reader.onload = (e) => {
                    addMessage(`[IMG] Analyzing image: "${prompt}"`, 'user');

                    // Show image preview
                    const img = document.createElement('img');
                    img.src = e.target.result;
                    img.className = 'image-preview';
                    chat.appendChild(img);
                    chat.scrollTop = chat.scrollHeight;

                    ws.send(JSON.stringify({
                        type: 'image',
                        image: e.target.result,
                        prompt: prompt
                    }));
                };
                reader.readAsDataURL(file);
            } else if (!file) {
                alert('Please select an image file.');
            } else {
                alert('WebSocket not connected.');
            }
        }

        function sendAudioMessage() {
            const input = document.getElementById('audioInput');
            const voiceSelect = document.getElementById('voiceSelect');
            const msg = input.value.trim();
            const voice = voiceSelect.value;

            if (msg && ws.readyState === WebSocket.OPEN) {
                addMessage(`[üîä] ${msg} (Voice: ${voice})`, 'user');

                document.getElementById('audioStatus').textContent = 'Processing audio...';

                ws.send(JSON.stringify({
                    type: 'text_audio',
                    message: msg,
                    voice: voice
                }));

                input.value = '';
            } else if (ws.readyState !== WebSocket.OPEN) {
                alert('Not connected. Please refresh the page.');
            }
        }

        function playAudioResponse(audioBase64) {
            try {
                // Convert base64 to Uint8Array
                const audioBytes = atob(audioBase64);
                const audioArray = new Uint8Array(audioBytes.length);
                for (let i = 0; i < audioBytes.length; i++) {
                    audioArray[i] = audioBytes.charCodeAt(i);
                }

                // Convert PCM to WAV format
                const wavBlob = convertPCMToWAV(audioArray);
                const audioUrl = URL.createObjectURL(wavBlob);

                // Play audio
                const audioPlayer = document.getElementById('audioPlayer');
                audioPlayer.src = audioUrl;
                audioPlayer.style.display = 'block';
                audioPlayer.play();

                document.getElementById('audioStatus').textContent = 'Playing audio response...';

                // Clean up URL after playing
                audioPlayer.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    document.getElementById('audioStatus').textContent = '';
                };
            } catch (error) {
                console.error('Audio playback error:', error);
                document.getElementById('audioStatus').textContent = 'Audio playback error: ' + error.message;
            }
        }

        function convertPCMToWAV(pcmData) {
            // PCM to WAV conversion
            const sampleRate = 24000; // Gemini Audio uses 24kHz
            const numChannels = 1;
            const bytesPerSample = 2; // 16-bit PCM

            const buffer = new ArrayBuffer(44 + pcmData.length);
            const view = new DataView(buffer);

            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };

            writeString(0, 'RIFF');
            view.setUint32(4, 36 + pcmData.length, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true); // PCM format
            view.setUint16(20, 1, true); // Audio format
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * numChannels * bytesPerSample, true);
            view.setUint16(32, numChannels * bytesPerSample, true);
            view.setUint16(34, 16, true); // Bits per sample
            writeString(36, 'data');
            view.setUint32(40, pcmData.length, true);

            // Copy PCM data
            for (let i = 0; i < pcmData.length; i++) {
                view.setUint8(44 + i, pcmData[i]);
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

        function speakText(text, voiceName) {
            if ('speechSynthesis' in window) {
                // Cancel any ongoing speech
                window.speechSynthesis.cancel();

                const utterance = new SpeechSynthesisUtterance(text);

                // Voice mapping
                const voiceMap = {
                    'Aoede': 'female',
                    'Charon': 'male',
                    'Fenrir': 'male',
                    'Kore': 'female',
                    'Puck': 'male'
                };

                // Try to find a suitable voice
                const voices = window.speechSynthesis.getVoices();
                const preferredGender = voiceMap[voiceName] || 'female';

                const suitableVoice = voices.find(voice =>
                    voice.lang.startsWith('en') &&
                    voice.name.toLowerCase().includes(preferredGender === 'female' ? 'female' : 'male')
                ) || voices.find(voice => voice.lang.startsWith('en'));

                if (suitableVoice) {
                    utterance.voice = suitableVoice;
                }

                utterance.rate = 1.0;
                utterance.pitch = 1.0;
                utterance.volume = 1.0;

                utterance.onend = () => {
                    document.getElementById('audioStatus').textContent = '';
                };

                utterance.onerror = (error) => {
                    console.error('Speech synthesis error:', error);
                    document.getElementById('audioStatus').textContent = 'TTS error';
                };

                window.speechSynthesis.speak(utterance);
            } else {
                console.warn('Speech synthesis not supported');
                document.getElementById('audioStatus').textContent = 'TTS not supported';
            }
        }

        function clearChat() {
            chat.innerHTML = '';
            addMessage('Chat cleared. Ready for new conversation!', 'ai');
        }

        // Audio recording variables for Live API streaming
        let mediaRecorder;
        let audioStream;
        let isRecording = false;
        let continuousMode = false;
        let voiceSessionActive = false;
        let processor;
        let recognition = null;
        let silenceTimer = null;
        let silenceCount = 0;
        let lastProcessedTranscript = '';

        async function startRecording() {
            const audioStatus = document.getElementById('recordingStatus');

            if (isRecording) {
                return; // Already recording
            }

            try {
                // Browser and security checks
                if (location.protocol !== 'https:' && location.hostname !== 'localhost' && location.hostname !== '127.0.0.1') {
                    audioStatus.textContent = '‚ö†Ô∏è Microphone requires HTTPS or localhost';
                    alert('Microphone access requires HTTPS or localhost.');
                    return;
                }

                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    audioStatus.textContent = '‚ö†Ô∏è Browser does not support microphone access';
                    alert('Your browser does not support microphone access.');
                    return;
                }

                // Voice session handled via WebSocket

                // Get microphone access with optimal settings for Gemini Live API
                audioStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 24000,  // Gemini Live API preferred rate
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });

                // Setup audio processing for real-time streaming
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 24000
                });

                const source = audioContext.createMediaStreamSource(audioStream);

                // Create script processor for real-time audio chunks
                processor = audioContext.createScriptProcessor(4096, 1, 1);

                processor.onaudioprocess = function(event) {
                    if (isRecording && voiceSessionActive) {
                        const inputBuffer = event.inputBuffer;
                        const inputData = inputBuffer.getChannelData(0);

                        // Convert Float32Array to Int16Array (PCM format for Gemini)
                        const pcmData = new Int16Array(inputData.length);
                        for (let i = 0; i < inputData.length; i++) {
                            pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
                        }

                        // Send PCM audio chunk to backend via WebSocket
                        if (ws && ws.readyState === WebSocket.OPEN) {
                            const encoded = btoa(String.fromCharCode(...new Uint8Array(pcmData.buffer)));
                            ws.send(JSON.stringify({
                                type: 'voice_audio_chunk',
                                audio: encoded
                            }));
                        }
                    }
                };

                source.connect(processor);
                processor.connect(audioContext.destination);

                isRecording = true;
                audioStatus.textContent = continuousMode ?
                    'üé§ Ïã§ÏãúÍ∞Ñ ÎåÄÌôî Ï§ë... ÏûêÏó∞Ïä§ÎüΩÍ≤å ÎßêÏîÄÌïòÏÑ∏Ïöî' :
                    'üé§ Recording... Speak now';

                console.log('Direct audio streaming to Gemini Live API started');

                // Setup speech recognition for voice commands
                if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                    recognition = new SpeechRecognition();
                    recognition.continuous = continuousMode;
                    recognition.interimResults = true;
                    recognition.lang = 'ko-KR';

                    recognition.onresult = (event) => {
                        const result = event.results[event.results.length - 1];
                        const transcript = result[0].transcript.trim();
                        const confidence = result[0].confidence || 0.8;

                        // Clear silence timer on speech detection
                        if (silenceTimer) {
                            clearTimeout(silenceTimer);
                            silenceTimer = null;
                        }

                        // Process on final result or high confidence partial
                        if (result.isFinal || (transcript.length > 2 && confidence > 0.75)) {
                            if (transcript !== lastProcessedTranscript && transcript && ws.readyState === WebSocket.OPEN) {
                                lastProcessedTranscript = transcript;

                                addMessage(`üé§ "${transcript}"`, 'user');
                                audioStatus.textContent = 'Processing with A2A agents...';

                                // Fast A2A processing
                                ws.send(JSON.stringify({
                                    type: 'text_audio',
                                    message: transcript,
                                    voice: 'Aoede',
                                    timestamp: performance.now(),
                                    continuous_mode: continuousMode
                                }));

                                // Don't stop if in continuous mode
                                if (!continuousMode) {
                                    stopRecording();
                                } else {
                                    audioStatus.textContent = 'Waiting for AI response...';
                                }
                            }
                        } else if (transcript.length > 0) {
                            audioStatus.textContent = `üé§ "${transcript}..." (listening)`;
                        }

                        // Auto-stop after silence in continuous mode
                        if (continuousMode && result.isFinal) {
                            silenceCount++;
                            silenceTimer = setTimeout(() => {
                                if (silenceCount > 2) { // After 2 final results with no new speech
                                    audioStatus.textContent = 'üé§ Listening for your next question...';
                                    silenceCount = 0;
                                }
                            }, 2000);
                        }
                    };

                    recognition.onerror = (event) => {
                        if (event.error !== 'no-speech' && event.error !== 'aborted') {
                            audioStatus.textContent = 'Speech error: ' + event.error;
                        }
                    };

                    recognition.onstart = () => {
                        const mode = continuousMode ? 'continuous conversation' : 'single question';
                        audioStatus.textContent = `üé§ Listening (${mode})... Speak now!`;
                    };

                    recognition.onend = () => {
                        if (isRecording && recognition && continuousMode) {
                            setTimeout(() => {
                                if (continuousMode && isRecording) {
                                    try {
                                        recognition.start();
                                    } catch (e) {
                                        console.error('Failed to restart recognition:', e);
                                    }
                                }
                            }, 100);
                        }
                    };

                    recognition.start();
                }

            } catch (error) {
                    console.error('Error accessing microphone:', error);
                    audioStatus.textContent = 'Microphone access denied: ' + error.message;
                }
        }

        async function toggleAudio() {
            const audioButton = document.getElementById('audioButton');

            if (!isRecording) {
                await startRecording();
            } else {
                stopRecording();
            }
        }

        function stopRecording() {
            const audioButton = document.getElementById('audioButton');
            const audioStatus = document.getElementById('recordingStatus');

            console.log('stopRecording called');

            if (isRecording) {
                // Stop speech recognition
                if (recognition) {
                    console.log('Stopping speech recognition...');
                    recognition.stop();
                    recognition = null;
                }

                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    mediaRecorder.stop();
                }

                if (mediaRecorder && mediaRecorder.stream) {
                    mediaRecorder.stream.getTracks().forEach(track => {
                        console.log('Stopping track:', track);
                        track.stop();
                    });
                }

                isRecording = false;
                audioButton.textContent = 'üé§ Start Recording';
                audioButton.style.background = '#4CAF50';
                audioStatus.textContent = 'Ready to record';
                console.log('Recording stopped');
            }
        }

        // A2A Agent Management Functions
        function switchAgent() {
            const agentSelect = document.getElementById('agentSelect');
            const selectedAgent = agentSelect.value;

            if (selectedAgent && ws.readyState === WebSocket.OPEN) {
                addMessage(`[ü§ñ] Switching to agent: ${selectedAgent}`, 'user');

                ws.send(JSON.stringify({
                    type: 'switch_agent',
                    agent_slug: selectedAgent
                }));

                document.getElementById('agentStatus').textContent = 'Switching agent...';
            } else if (ws.readyState !== WebSocket.OPEN) {
                alert('Not connected. Please refresh the page.');
            }
        }

        function requestAgentList() {
            if (ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                    type: 'list_agents'
                }));

                document.getElementById('agentStatus').textContent = 'Refreshing agent list...';
            } else {
                console.warn('WebSocket not connected, cannot request agent list');
            }
        }

        function updateCurrentAgent(agentSlug, agentName) {
            const agentStatusEl = document.getElementById('agentStatus');
            const agentInfoEl = document.getElementById('agentInfo');

            agentStatusEl.textContent = `Current: ${agentName}`;

            // Update agent info based on known agent configurations
            const agentConfigs = {
                'general-worker': 'Voice: Warm and helpful general assistant',
                'flight-specialist': 'Voice: Professional travel specialist',
                'hotel-specialist': 'Voice: Friendly hospitality specialist',
                'travel-assistant': 'Voice: Energetic travel coordinator'
            };

            agentInfoEl.textContent = agentConfigs[agentSlug] || 'Voice: AI Assistant';

            // Update dropdown selection
            const agentSelect = document.getElementById('agentSelect');
            agentSelect.value = agentSlug;
        }

        function updateAgentsList(agents, currentAgent) {
            const agentSelect = document.getElementById('agentSelect');

            // Clear existing options
            agentSelect.innerHTML = '';

            // Add agents to dropdown
            for (const [slug, info] of Object.entries(agents)) {
                const option = document.createElement('option');
                option.value = slug;
                option.textContent = `${info.name} (${info.voice_style})`;
                agentSelect.appendChild(option);
            }

            // Set current agent
            agentSelect.value = currentAgent;
            updateCurrentAgent(currentAgent, agents[currentAgent]?.name || 'Unknown Agent');
        }

        function updateAgentInfo(agentData) {
            const agentInfoEl = document.getElementById('agentInfo');
            agentInfoEl.textContent = `${agentData.agent_description} | Capabilities: ${agentData.capabilities?.join(', ') || 'Standard'}`;
        }

        // Start audio capture using Context7 SEPARATED worklet pattern (from continuous_voice.html)
        async function startAudioCapture() {
            try {
                // Set voice mode active to prevent TTS interference
                isVoiceActive = true;
                // Official Gemini Live API audio configuration
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 16000,  // Official Gemini Live API input standard
                        echoCancellation: true,
                        noiseSuppression: true,
                        channelCount: 1
                    }
                });

                // Input: 16kHz, Output: 24kHz (per official Gemini cookbook)
                audioContext = new AudioContext({sampleRate: 24000});

                // Setup analyser for visualization
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                microphone.connect(analyser);

                // Context7 pattern: Load separated AudioWorklet modules
                await audioContext.audioWorklet.addModule(URL.createObjectURL(
                    new Blob([inputWorkletCode], {type: 'text/javascript'})
                ));
                await audioContext.audioWorklet.addModule(URL.createObjectURL(
                    new Blob([outputWorkletCode], {type: 'text/javascript'})
                ));

                // Create SEPARATED processors
                inputProcessor = new AudioWorkletNode(audioContext, 'input-processor');
                outputProcessor = new AudioWorkletNode(audioContext, 'output-processor');

                // Real-time 12.5ms batch audio transmission
                inputProcessor.port.onmessage = (event) => {
                    if ('audio_in' in event.data && ws && ws.readyState === WebSocket.OPEN) {
                        // Context7 base64 encoding
                        const encoded = btoa(String.fromCharCode(
                            ...Array.from(new Uint8Array(event.data.audio_in))
                        ));

                        ws.send(JSON.stringify({
                            type: 'voice_audio_chunk',
                            audio: encoded
                        }));
                    }
                };

                // Connect SEPARATED audio pipeline (Context7 pattern)
                microphone.connect(inputProcessor);
                // INPUT: microphone -> inputProcessor (NO destination!)
                // OUTPUT: outputProcessor -> destination (separate path!)
                outputProcessor.connect(audioContext.destination);

                // Start visualization (from continuous_voice.html)
                visualizeAudio();

                addMessage('Official Gemini Live API (16kHz input/24kHz output) active with 50ms Context7 streaming - speak freely!', 'ai');

            } catch (error) {
                console.error('Error accessing microphone:', error);
                addMessage('Failed to access microphone', 'ai');
                stopConversation();
            }
        }

        // One-click continuous conversation (Enhanced with real-time streaming)
        async function startContinuousConversation() {
            const audioButton = document.getElementById('audioButton');
            const stopButton = document.getElementById('stopButton');
            const audioStatus = document.getElementById('recordingStatus');

            if (isRecording) {
                return; // Already recording
            }

            // Check WebSocket connection
            if (!ws || ws.readyState !== WebSocket.OPEN) {
                audioStatus.textContent = 'WebSocket not connected. Please refresh the page.';
                return;
            }

            // Enable continuous mode automatically
            continuousMode = true;
            isRecording = true;

            // Update UI
            audioButton.style.display = 'none';
            stopButton.style.display = 'inline-block';
            audioStatus.textContent = 'Starting real-time voice session...';

            // Start continuous voice session (from continuous_voice.html pattern)
            ws.send(JSON.stringify({
                type: 'start_voice_session'
            }));
        }

        function stopConversation() {
            const audioButton = document.getElementById('audioButton');
            const stopButton = document.getElementById('stopButton');
            const audioStatus = document.getElementById('recordingStatus');

            // Disable continuous mode
            continuousMode = false;
            isRecording = false;
            isVoiceActive = false;  // Re-enable TTS system

            // Clear SEPARATED audio worklets (from continuous_voice.html)
            if (inputProcessor) {
                inputProcessor.disconnect();
                inputProcessor = null;
            }
            if (outputProcessor) {
                outputProcessor.port.postMessage({'clear': ''});
                outputProcessor.disconnect();
                outputProcessor = null;
            }

            // Close audio context
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }

            // Send stop command to backend
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                    type: 'stop_voice_session'
                }));
            }

            // Stop old recording if exists
            stopRecording();

            // Update UI
            audioButton.style.display = 'inline-block';
            stopButton.style.display = 'none';
            audioStatus.textContent = 'Click to start continuous voice conversation';
            
            addMessage('Real-time conversation ended', 'ai');
        }

        // Visualize audio (from continuous_voice.html)
        function visualizeAudio() {
            if (!analyser || (!isRecording && !isVoiceActive)) return;

            const canvas = document.getElementById('visualizer');
            const ctx = canvas.getContext('2d');
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            function draw() {
                if (!isRecording && !isVoiceActive) return;

                requestAnimationFrame(draw);

                analyser.getByteFrequencyData(dataArray);

                ctx.fillStyle = 'rgb(0, 0, 0)';
                ctx.fillRect(0, 0, canvas.width, canvas.height);

                const barWidth = (canvas.width / bufferLength) * 2.5;
                let barHeight;
                let x = 0;

                for (let i = 0; i < bufferLength; i++) {
                    barHeight = dataArray[i] / 2;

                    ctx.fillStyle = `rgb(${barHeight + 100}, 50, ${250 - barHeight})`;
                    ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);

                    x += barWidth + 1;
                }
            }

            canvas.width = canvas.offsetWidth;
            canvas.height = canvas.offsetHeight;
            draw();
        }

        function restartListening() {
            if (recognition && continuousMode && isRecording) {
                try {
                    recognition.start();
                } catch (e) {
                    // If already running, don't worry
                    if (e.name !== 'InvalidStateError') {
                        console.error('Failed to restart recognition:', e);
                    }
                }
            }
        }

        // Optimized stopRecording function
        function stopRecording() {
            const audioButton = document.getElementById('audioButton');
            const stopButton = document.getElementById('stopButton');
            const audioStatus = document.getElementById('recordingStatus');

            console.log('stopRecording called');

            if (isRecording) {
                // Clear any timers
                if (silenceTimer) {
                    clearTimeout(silenceTimer);
                    silenceTimer = null;
                }

                // Stop speech recognition
                if (recognition) {
                    recognition.stop();
                    recognition = null;
                }

                // Stop media recorder
                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    mediaRecorder.stop();
                }

                // Stop all tracks
                if (mediaRecorder && mediaRecorder.stream) {
                    mediaRecorder.stream.getTracks().forEach(track => track.stop());
                }

                isRecording = false;

                // Reset UI to default state
                audioButton.textContent = 'üé§ Start Real-time Conversation';
                audioButton.style.background = '#667eea';
                audioButton.style.display = 'inline-block';
                stopButton.style.display = 'none';
                audioStatus.textContent = 'Click to start continuous voice conversation';
            }
        }
    </script>
</body>
</html>