<!DOCTYPE html>
<html>
<head>
    <title>{{ page_title }}</title>
    <meta charset="UTF-8">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .container {
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
        }
        .header h1 {
            color: #333;
            margin-bottom: 10px;
        }
        .header p {
            color: #666;
            font-size: 14px;
        }
        .chat {
            border: 2px solid #e0e0e0;
            height: 500px;
            overflow-y: auto;
            padding: 20px;
            margin: 20px 0;
            background: #f9f9f9;
            border-radius: 15px;
        }
        .message {
            margin: 15px 0;
            padding: 15px;
            border-radius: 10px;
            max-width: 80%;
            animation: fadeIn 0.3s;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .user {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin-left: auto;
            text-align: right;
        }
        .ai {
            background: white;
            border: 1px solid #e0e0e0;
            margin-right: auto;
        }

        @keyframes pulse {
            0% { transform: scale(1); }
            50% { transform: scale(1.05); }
            100% { transform: scale(1); }
        }

        .a2a_confirmation {
            background: #ffeb3b;
            border: 2px solid #ff9800;
            margin-right: auto;
            box-shadow: 0 4px 8px rgba(255, 152, 0, 0.3);
            animation: pulse 2s infinite;
        }
        .delegation {
            background: linear-gradient(135deg, #ff9a8b 0%, #a8e6cf 100%);
            color: #2c3e50;
            border: 2px solid #27ae60;
            margin-right: auto;
            font-weight: bold;
            box-shadow: 0 4px 15px rgba(39, 174, 96, 0.3);
            animation: pulse 2s ease-in-out;
        }
        @keyframes pulse {
            0% { transform: scale(1); box-shadow: 0 4px 15px rgba(39, 174, 96, 0.3); }
            50% { transform: scale(1.02); box-shadow: 0 6px 20px rgba(39, 174, 96, 0.5); }
            100% { transform: scale(1); box-shadow: 0 4px 15px rgba(39, 174, 96, 0.3); }
        }
        .controls {
            margin: 20px 0;
            padding: 20px;
            background: #f5f5f5;
            border-radius: 15px;
        }
        .control-group {
            margin-bottom: 20px;
        }
        .control-group h3 {
            color: #333;
            margin-bottom: 10px;
        }
        input, button {
            padding: 12px 20px;
            margin: 5px;
            border: 2px solid #e0e0e0;
            border-radius: 8px;
            font-size: 14px;
            transition: all 0.3s;
        }
        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            cursor: pointer;
            font-weight: bold;
        }
        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }
        #messageInput { width: 60%; }
        #imagePrompt { width: 50%; }
        .status {
            padding: 10px 20px;
            margin: 10px 0;
            border-radius: 8px;
            text-align: center;
            font-weight: bold;
            transition: all 0.3s;
        }
        .connected {
            background: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }
        .disconnected {
            background: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }
        .image-preview {
            max-width: 300px;
            margin: 10px 0;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        .info-badge {
            display: inline-block;
            padding: 4px 8px;
            background: #f0f0f0;
            border-radius: 4px;
            font-size: 12px;
            color: #666;
            margin-left: 10px;
        }
        .capabilities {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin-top: 10px;
        }
        .capability {
            padding: 5px 15px;
            background: #e3f2fd;
            color: #1976d2;
            border-radius: 20px;
            font-size: 12px;
            font-weight: bold;
        }

        /* Continuous Voice Audio Styles */
        .voice-controls {
            background: #f8f9fa;
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            border: 2px solid #e0e0e0;
        }
        .voice-status {
            text-align: center;
            padding: 15px;
            border-radius: 10px;
            margin-bottom: 20px;
            font-weight: bold;
            transition: all 0.3s ease;
        }
        .voice-status.inactive {
            background: #f0f0f0;
            color: #666;
        }
        .voice-status.active {
            background: linear-gradient(135deg, #667eea, #764ba2);
            color: white;
            animation: pulse 2s infinite;
        }
        .visualizer {
            height: 80px;
            background: #000;
            border-radius: 10px;
            margin-bottom: 15px;
            display: flex;
            align-items: center;
            justify-content: center;
            position: relative;
            overflow: hidden;
        }
        .visualizer canvas {
            width: 100%;
            height: 100%;
        }
        .voice-button-group {
            display: flex;
            gap: 10px;
            justify-content: center;
        }
        .voice-button {
            flex: 1;
            max-width: 200px;
            padding: 12px 20px;
            border: none;
            border-radius: 8px;
            font-weight: bold;
            cursor: pointer;
            transition: all 0.3s ease;
        }
        .start-voice-btn {
            background: #4CAF50;
            color: white;
        }
        .start-voice-btn:hover {
            background: #45a049;
            transform: translateY(-2px);
        }
        .stop-voice-btn {
            background: #f44336;
            color: white;
        }
        .stop-voice-btn:hover {
            background: #da190b;
            transform: translateY(-2px);
        }
        .voice-button:disabled {
            opacity: 0.5;
            cursor: not-allowed;
            transform: none !important;
        }

        /* A2A Delegation Flow Styles */
        .delegation-section {
            background: white;
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            box-shadow: 0 4px 12px rgba(0,0,0,0.1);
            border: 2px solid #e3f2fd;
        }
        .delegation-flow {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin: 15px 0;
            padding: 15px;
            background: #f8f9fa;
            border-radius: 10px;
        }
        .agent-indicator {
            display: flex;
            flex-direction: column;
            align-items: center;
            min-width: 120px;
        }
        .agent-icon {
            width: 50px;
            height: 50px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 20px;
            margin-bottom: 8px;
            transition: all 0.3s ease;
        }
        .test-agent-icon { background: #4CAF50; color: white; }
        .flight-specialist-icon { background: #2196F3; color: white; }
        .hotel-specialist-icon { background: #FF9800; color: white; }
        .general-worker-icon { background: #9C27B0; color: white; }
        .agent-name {
            font-weight: bold;
            font-size: 12px;
            text-align: center;
            margin-bottom: 5px;
        }
        .agent-status {
            padding: 4px 8px;
            border-radius: 12px;
            font-size: 10px;
            font-weight: bold;
            text-transform: uppercase;
            transition: all 0.3s ease;
        }
        .status-ready { background: #d4edda; color: #155724; }
        .status-thinking { background: #fff3cd; color: #856404; }
        .status-active { background: #cce5ff; color: #004085; }
        .status-delegated { background: #f8d7da; color: #721c24; }

        .delegation-arrow {
            font-size: 24px;
            color: #6c757d;
            transition: all 0.3s ease;
            margin: 0 10px;
        }
        .delegation-arrow.active {
            color: #28a745;
            transform: scale(1.3);
            animation: pulse 1s ease-in-out;
        }
        @keyframes pulse {
            0% { transform: scale(1.3); }
            50% { transform: scale(1.5); }
            100% { transform: scale(1.3); }
        }

        .communication-log {
            background: #f8f9fa;
            border: 1px solid #dee2e6;
            border-radius: 8px;
            padding: 12px;
            margin: 15px 0;
            max-height: 120px;
            overflow-y: auto;
            font-size: 12px;
        }
        .log-entry {
            margin: 3px 0;
            padding: 4px 8px;
            border-radius: 4px;
            background: white;
            border-left: 3px solid #007bff;
            font-size: 11px;
        }
        .log-delegation { border-left-color: #28a745; }
        .log-response { border-left-color: #17a2b8; }
        .log-error { border-left-color: #dc3545; }

        .current-agent-display {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 10px 15px;
            border-radius: 8px;
            margin: 10px 0;
            text-align: center;
            font-weight: bold;
            box-shadow: 0 2px 8px rgba(102, 126, 234, 0.3);
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{{ page_title }}</h1>
            <p>Model: {{ model_name }} | Django + Channels WebSocket</p>
            <div class="capabilities">
                <span class="capability">Text</span>
                <span class="capability">Image</span>
                <span class="capability">Video (Soon)</span>
                <span class="capability">Audio (Input)</span>
            </div>
        </div>

        <div id="status" class="status disconnected">Connecting to Live API...</div>

        <!-- A2A Delegation Flow Indicator -->
        <div class="delegation-section">
            <h3 style="margin: 0 0 15px 0; color: #333; font-size: 16px;">A2A Agent Delegation Flow</h3>

            <div class="current-agent-display" id="currentAgentDisplay">
                Current Active Agent: <span id="currentAgentName">Test Agent (Coordinator)</span>
            </div>

            <div class="delegation-flow">
                <div class="agent-indicator">
                    <div class="agent-icon test-agent-icon" id="testAgentIcon">üß†</div>
                    <div class="agent-name">Test Agent</div>
                    <div class="agent-status status-ready" id="testAgentStatus">Ready</div>
                </div>

                <div class="delegation-arrow" id="delegationArrow">‚û°Ô∏è</div>

                <div class="agent-indicator">
                    <div class="agent-icon flight-specialist-icon" id="flightAgentIcon">‚úàÔ∏è</div>
                    <div class="agent-name">Flight Specialist</div>
                    <div class="agent-status status-ready" id="flightAgentStatus">Ready</div>
                </div>

                <div class="delegation-arrow" id="delegationArrow2">‚û°Ô∏è</div>

                <div class="agent-indicator">
                    <div class="agent-icon hotel-specialist-icon" id="hotelAgentIcon">üè®</div>
                    <div class="agent-name">Hotel Specialist</div>
                    <div class="agent-status status-ready" id="hotelAgentStatus">Ready</div>
                </div>
            </div>

            <div class="communication-log" id="delegationLog">
                <strong>Real-time A2A Communication Log</strong>
                <div id="logEntries">
                    <div class="log-entry">System initialized - Agents ready for delegation</div>
                </div>
            </div>
        </div>

        <!-- Main Chat Window -->
        <div id="chat" class="chat"></div>

        <!-- A2A Agent Communication Window -->
        <div class="delegation-section">
            <h3 style="margin: 0 0 15px 0; color: #333; font-size: 16px;">A2A Agent Communication Window</h3>
            <div id="a2aChat" class="chat" style="height: 300px; background: #f0f8ff; border: 2px solid #007bff;">
                <div class="message system-message" style="background: #e3f2fd; color: #1976d2; text-align: center; margin: 10px; padding: 10px; border-radius: 8px;">
                    A2A Agent-to-Agent communication will appear here in real-time
                </div>
            </div>
        </div>

        <div class="controls">
            <div class="control-group">
                <h3>Image Analysis</h3>
                <input type="file" id="imageInput" accept="image/*">
                <input type="text" id="imagePrompt" placeholder="Ask about the image..." value="What do you see in this image?">
                <button onclick="uploadImage()">Analyze Image</button>
            </div>

            <div class="control-group">
                <h3>A2A Agent Selection</h3>
                <select id="agentSelect" style="margin: 5px; width: 60%;">
                    <option value="test-agent" selected>Test Agent (Coordinator) - Voice: Charon (Male, Confident)</option>
                    <option value="flight-specialist">Flight Specialist - Voice: Aoede (Female, Professional)</option>
                    <option value="hotel-specialist">Hotel Specialist - Voice: Kore (Female, Friendly)</option>
                    <option value="general-worker">General Worker - Voice: Leda (Female, Warm)</option>
                </select>
                <button onclick="switchAgent()">Switch Agent</button>
                <button onclick="requestAgentList()">Refresh Agents</button>
                <div id="agentStatus" style="margin: 10px 0; font-size: 12px; color: #666;">Current: Test Agent (Coordinator)</div>
                <div id="agentInfo" style="margin: 10px 0; font-size: 11px; color: #888; font-style: italic;">Voice: Charon (Male) - Confident coordinator voice for delegation</div>
            </div>

            <div class="control-group">
                <h3>Audio Chat (TTS)</h3>
                <input type="text" id="audioInput" placeholder="Type message for voice response..." style="width: 50%;">
                <select id="voiceSelect" style="margin: 5px;">
                    <option value="Aoede" selected>Aoede (Female - Greek muse)</option>
                    <option value="Kore">Kore (Female - Greek goddess)</option>
                    <option value="Leda">Leda (Female)</option>
                    <option value="Charon">Charon (Male)</option>
                    <option value="Fenrir">Fenrir (Male)</option>
                </select>
                <button onclick="sendAudioMessage()">Get Voice Response</button>
                <div id="audioStatus" style="margin: 10px 0; font-size: 12px; color: #666;"></div>
                <audio id="audioPlayer" controls style="display: none; margin: 10px 0;"></audio>
            </div>

            <div class="control-group">
                <h3>Voice Conversation</h3>
                
                <!-- Audio Visualizer (from continuous_voice.html) -->
                <div style="height: 80px; background: #000; border-radius: 10px; margin: 15px 0; display: flex; align-items: center; justify-content: center; position: relative; overflow: hidden;">
                    <canvas id="visualizer" style="width: 100%; height: 100%;"></canvas>
                </div>
                
                <button id="audioButton" onclick="startContinuousConversation()">Start Real-time Conversation</button>
                <button id="stopButton" onclick="stopConversation()" style="margin-left: 10px; display: none; background: #dc3545;">Stop</button>
                <div id="recordingStatus" style="margin: 10px 0; font-size: 12px; color: #666;">Click to start continuous voice conversation</div>
                <div style="font-size: 11px; color: #888; margin-top: 5px;">
                    One-click continuous conversation: Just speak naturally, AI responds automatically
                </div>
            </div>

            <div class="control-group">
                <h3>Text Chat</h3>
                <input type="text" id="messageInput" placeholder="Type your message..." onkeypress="if(event.key==='Enter') sendMessage()">
                <button onclick="sendMessage()">Send</button>
                <button onclick="clearChat()">Clear</button>
            </div>
        </div>
    </div>

    <script>
        console.log('Starting WebSocket connection to:', '{{ websocket_url }}');
        const ws = new WebSocket('{{ websocket_url }}');
        const chat = document.getElementById('chat');
        const status = document.getElementById('status');

        // Continuous Voice Variables (from continuous_voice.html)
        let audioContext = null;
        let analyser = null;
        let microphone = null;
        let isVoiceActive = false;
        let inputProcessor = null;
        let outputProcessor = null;

        // Official Context7 Cookbook Pattern - AudioWorklet for real-time streaming
        const inputWorkletCode = `
        class InputProcessor extends AudioWorkletProcessor {
            constructor() {
                super();
                this._out = [];
                this._out_len = 0;
                console.log("Official Context7 InputProcessor initialized", this);

                this.port.postMessage({
                    debug: "Input processor ready with official Context7 pattern!",
                });
            }

            encodeAudio(input) {
                const channel = input[0];
                const data = new ArrayBuffer(2 * channel.length);
                const view = new DataView(data);
                for (let i = 0; i < channel.length; i++) {
                    view.setInt16(2*i, channel[i] * 32767, true);
                }
                return data;
            }

            process(inputs, outputs, parameters) {
                // Official Context7 Pattern: 50ms batch transmission
                if (inputs[0] && inputs[0][0]) {
                    let data = this.encodeAudio(inputs[0]);
                    this._out.push(data);
                    this._out_len += data.byteLength;

                    // Official Gemini Cookbook: sampleRate/20 = 50ms batches
                    // Note: Input is 16kHz for microphone, but use actual sampleRate
                    if (this._out_len > (2 * 16000 / 20)) {
                        let concat = new Uint8Array(this._out_len);
                        let idx = 0;
                        for (let a of this._out) {
                            concat.set(new Uint8Array(a), idx);
                            idx += a.byteLength;
                        }
                        this._out = [];
                        this._out_len = 0;
                        this.port.postMessage({
                            'audio_in': concat.buffer,
                        });
                    }
                }
                return true;
            }
        }

        registerProcessor('input-processor', InputProcessor);
        `;

        // Context7 Cookbook OUTPUT PROCESSOR - seamless playback
        const outputWorkletCode = `
        class OutputProcessor extends AudioWorkletProcessor {
            constructor() {
                super();
                this._queue = [];
                this.port.onmessage = (event) => {
                    if ('enqueue' in event.data) {
                        this.enqueueAudio(event.data.enqueue);
                    }
                    if ('clear' in event.data) {
                        this.clearAudio();
                    }
                };
                console.log("Context7 OutputProcessor initialized");
                this.port.postMessage({
                    debug: "Output processor ready for seamless playback!",
                });
            }

            enqueueAudio(input) {
                // Context7 pattern: Convert PCM bytes to Float32
                let view = new DataView(input);
                let floats = [];
                for (let i = 0; i < input.byteLength; i += 2) {
                    floats.push(view.getInt16(i, true) / 32768.0);
                }
                this._queue.push(Float32Array.from(floats));
            }

            dequeueIntoBuffer(output) {
                // Context7 seamless buffer management
                let idx = 0;
                while (idx < output.length) {
                    if (this._queue.length === 0) {
                        // Fill remaining with silence to prevent clicks
                        output.fill(0, idx);
                        return;
                    }
                    let input = this._queue[0];
                    if (input.length == 0) {
                        this._queue.shift();
                        continue;
                    }
                    let n = Math.min(input.length, output.length - idx);
                    output.set(input.subarray(0, n), idx);
                    this._queue[0] = input.subarray(n);
                    idx += n;
                }
            }

            clearAudio() {
                this._queue = [];
            }

            process(inputs, outputs, parameters) {
                // Context7 OUTPUT: seamless audio playback
                if (outputs[0] && outputs[0][0]) {
                    this.dequeueIntoBuffer(outputs[0][0]);
                    // Copy to stereo channels
                    for (let i = 1; i < outputs[0].length; i++) {
                        const src = outputs[0][0];
                        const dst = outputs[0][i];
                        dst.set(src.subarray(0, dst.length));
                    }
                }
                return true;
            }
        }

        registerProcessor('output-processor', OutputProcessor);
        `;

        ws.onopen = () => {
            console.log('WebSocket connected!');
            status.textContent = 'Connected to Django + Gemini Live API + A2A Agents';
            status.className = 'status connected';

            // Initialize delegation UI
            addDelegationLogEntry('WebSocket connected - A2A delegation system ready', 'info');
            updateAgentStatus('test-agent', 'Active');

            // Request initial agent list
            setTimeout(() => {
                requestAgentList();
            }, 500);
        };

        ws.onclose = (event) => {
            console.log('WebSocket closed:', event.code, event.reason);
            status.textContent = 'Disconnected';
            status.className = 'status disconnected';
        };

        ws.onerror = (error) => {
            console.error('WebSocket error details:', error);
            status.textContent = 'Connection Error';
            status.className = 'status disconnected';
        };

        // Streaming variables
        let currentStreamingMessage = null;
        let streamingMessageElement = null;

        // A2A Delegation UI Variables
        let currentActiveAgent = 'test-agent';
        const agentVoiceMap = {
            'test-agent': 'Charon',
            'flight-specialist': 'Aoede',
            'hotel-specialist': 'Kore',
            'general-worker': 'Leda'
        };

        // A2A Delegation UI Functions
        function updateAgentStatus(agentSlug, status) {
            const statusMap = {
                'test-agent': 'testAgentStatus',
                'flight-specialist': 'flightAgentStatus',
                'hotel-specialist': 'hotelAgentStatus'
            };

            const statusElement = document.getElementById(statusMap[agentSlug]);
            if (statusElement) {
                statusElement.textContent = status;
                statusElement.className = `agent-status status-${status.toLowerCase().replace(' ', '-')}`;
            }
        }

        function animateDelegationArrow(fromAgent, toAgent) {
            const arrow = document.getElementById('delegationArrow');
            const arrow2 = document.getElementById('delegationArrow2');

            // Determine which arrow to animate based on delegation flow
            let targetArrow = arrow;
            if (fromAgent === 'test-agent' && toAgent === 'hotel-specialist') {
                // Use second arrow for hotel delegation
                targetArrow = arrow2;
            }

            if (targetArrow) {
                targetArrow.classList.add('active');
                setTimeout(() => targetArrow.classList.remove('active'), 2000);
            }
        }

        function addDelegationLogEntry(message, type = 'info') {
            const logEntries = document.getElementById('logEntries');
            const timestamp = new Date().toLocaleTimeString();
            const entry = document.createElement('div');

            let logClass = 'log-entry';
            if (type === 'delegation') logClass += ' log-delegation';
            else if (type === 'response') logClass += ' log-response';
            else if (type === 'error') logClass += ' log-error';

            entry.className = logClass;
            entry.innerHTML = `<small>${timestamp}</small> - ${message}`;
            logEntries.appendChild(entry);

            // Keep only last 8 entries
            while (logEntries.children.length > 8) {
                logEntries.removeChild(logEntries.firstChild);
            }

            // Scroll to bottom
            const delegationLog = document.getElementById('delegationLog');
            delegationLog.scrollTop = delegationLog.scrollHeight;
        }

        function updateCurrentAgentDisplay(agentSlug, agentName) {
            const display = document.getElementById('currentAgentName');
            const voiceInfo = agentVoiceMap[agentSlug] || 'Aoede';

            if (display) {
                display.textContent = `${agentName} (Voice: ${voiceInfo})`;
            }

            // Update current agent variable
            currentActiveAgent = agentSlug;

            // Update voice selector to match agent
            const voiceSelect = document.getElementById('voiceSelect');
            if (voiceSelect && agentVoiceMap[agentSlug]) {
                voiceSelect.value = agentVoiceMap[agentSlug];
            }
        }

        function processDelegationFlow(data) {
            if (data.delegation_occurred || data.agent_switched) {
                const fromAgent = data.old_agent || currentActiveAgent;
                const toAgent = data.new_agent || data.agent_slug;
                const agentName = data.agent_name || 'Specialist Agent';

                // Update status indicators
                updateAgentStatus(fromAgent, 'Delegated');
                updateAgentStatus(toAgent, 'Active');

                // Animate delegation arrow
                animateDelegationArrow(fromAgent, toAgent);

                // Add log entries
                addDelegationLogEntry(`Delegation: ${fromAgent} to ${toAgent}`, 'delegation');
                addDelegationLogEntry(`${agentName} is now handling the request`, 'response');

                // Add A2A communication messages to separate window
                addA2AMessage(` DELEGATION STARTED: ${fromAgent} to${toAgent}`, 'system', 'A2A Protocol');
                addA2AMessage(` Request forwarded to ${agentName}`, 'ai', fromAgent);
                addA2AMessage(` ${agentName} accepting request...`, 'user', toAgent);

                // Update current agent display
                updateCurrentAgentDisplay(toAgent, agentName);

                // Reset previous agent status after delay
                setTimeout(() => {
                    updateAgentStatus(fromAgent, 'Ready');
                }, 3000);
            }
        }

        ws.onmessage = (event) => {
            const data = JSON.parse(event.data);
            console.log('WebSocket received:', data); // DEBUG

            if (data.type === 'connection') {
                addMessage(`Connected! Model: ${data.model}`, 'ai');
            } else if (data.type === 'streaming_chunk') {
                // Handle real-time streaming chunks (Gemini)
                if (!currentStreamingMessage) {
                    // Create new streaming message
                    currentStreamingMessage = '';
                    streamingMessageElement = document.createElement('div');
                    streamingMessageElement.className = 'message ai';
                    streamingMessageElement.innerHTML = '[STREAM] ';
                    chat.appendChild(streamingMessageElement);
                }

                if (data.chunk) {
                    currentStreamingMessage += data.chunk;
                    streamingMessageElement.innerHTML = `[STREAM] ${currentStreamingMessage}`;
                    chat.scrollTop = chat.scrollHeight;
                }

                if (data.is_final) {
                    // Finalize streaming message
                    streamingMessageElement.innerHTML = `[TXT] ${currentStreamingMessage}`;
                    currentStreamingMessage = null;
                    streamingMessageElement = null;
                }
            } else if (data.type === 'a2a_streaming_chunk') {
                // Handle real-time A2A agent streaming chunks
                if (!currentStreamingMessage) {
                    // Create new A2A streaming message
                    currentStreamingMessage = '';
                    streamingMessageElement = document.createElement('div');
                    streamingMessageElement.className = 'message ai';
                    streamingMessageElement.innerHTML = `[AGENT ${data.agent_slug}] `;
                    chat.appendChild(streamingMessageElement);
                }

                if (data.chunk) {
                    currentStreamingMessage += data.chunk;
                    streamingMessageElement.innerHTML = `[AGENT ${data.agent_slug}] ${currentStreamingMessage}`;
                    chat.scrollTop = chat.scrollHeight;
                }
            } else if (data.type === 'ready_for_next') {
                // Handle continuous mode ready signal
                if (data.continuous_mode && continuousMode && isRecording) {
                    const audioStatus = document.getElementById('recordingStatus');
                    audioStatus.textContent = 'VOICE Ready for your next question... Speak now!';

                    // Optionally show visual indicator
                    setTimeout(() => {
                        if (continuousMode && isRecording) {
                            audioStatus.textContent = 'Listening for your next question...';
                        }
                    }, 1000);
                }
            } else if (data.type === 'processing_started') {
                // Handle processing acknowledgment
                if (data.continuous_mode) {
                    const audioStatus = document.getElementById('recordingStatus');
                    audioStatus.textContent = `VOICE Processing: "${data.message}"...`;
                }
            } else if (data.type === 'response' || data.type === 'image_response') {
                const time = data.response_time ? ` <span class="info-badge">${data.response_time.toFixed(2)}s</span>` : '';
                let typeIcon = '[TXT]';
                if (data.type === 'image_response') typeIcon = '[IMG]';
                else if (data.analysis_type === 'image_regular_api') typeIcon = '[IMG]';
                else if (data.analysis_type === 'audio_live_api') typeIcon = '[AUD]';
                else if (data.analysis_type === 'video_frame_analysis') typeIcon = '[VID]';

                // Process delegation flow if present
                processDelegationFlow(data);

                // Show agent name in message if available
                const agentName = data.agent_name ? ` (${data.agent_name})` : '';
                addMessage(`${typeIcon}${agentName} ${data.message}${time}`, 'ai');

                // Check for delegation (matching worker_test.html logic)
                if (data.delegation && data.delegation.occurred) {
                    // Add system message about delegation
                    addMessage('[A2A] Worker communication occurred', 'ai');

                    // Log the communication
                    addDelegationLogEntry(`${data.delegation.original_agent} to ${data.delegation.specialist_agent}: "${data.delegation.reason}"`, 'delegation');
                    animateDelegationArrow(data.delegation.original_agent, data.delegation.specialist_agent);

                    // Show delegation in A2A chat window with enhanced formatting
                    addA2AMessage(`A2A DELEGATION START`, 'system', 'A2A Protocol');
                    addA2AMessage(`From Agent: ${data.delegation.original_agent}`, 'ai', data.delegation.original_agent);
                    addA2AMessage(`To Agent: ${data.delegation.specialist_agent}`, 'ai', 'System');
                    addA2AMessage(`User Request: "${data.delegation.reason}"`, 'user', 'User Request');

                    // Show specialist response
                    if (data.delegation.specialist_response) {
                        addA2AMessage(`Specialist Response: ${data.delegation.specialist_response}`, 'ai', data.delegation.specialist_agent);
                    }

                    addA2AMessage(`A2A DELEGATION COMPLETE`, 'system', 'A2A Protocol');

                    // Update agent statuses
                    updateAgentStatus(data.delegation.original_agent, 'Delegated');
                    updateAgentStatus(data.delegation.specialist_agent, 'Active');

                    // Reset status after delay
                    setTimeout(() => {
                        updateAgentStatus(data.delegation.original_agent, 'Ready');
                        updateAgentStatus(data.delegation.specialist_agent, 'Ready');
                    }, 3000);
                }

                // Update agent status
                if (data.agent_slug) {
                    updateAgentStatus(data.agent_slug, 'Ready');
                }
            } else if (data.type === 'audio_response') {
                const time = data.response_time ? ` <span class="info-badge">${data.response_time.toFixed(2)}s</span>` : '';
                const transcript = data.transcript || 'No response generated';
                const agentInfo = data.agent_name ? ` (${data.agent_name})` : '';

                // Process delegation flow for audio responses too
                processDelegationFlow(data);

                addMessage(`[üîä] ${transcript}${time}${agentInfo}`, 'ai');

                // Update agent status
                if (data.agent_slug) {
                    updateAgentStatus(data.agent_slug, 'Ready');
                }

                // Play audio if available, otherwise use browser TTS (only in non-continuous voice mode)
                if (!isVoiceActive) {
                    if (data.audio && data.success) {
                        playAudioResponse(data.audio);
                    } else if (transcript && transcript !== 'No response generated') {
                        // Fallback to browser TTS
                        speakText(transcript, data.voice || 'Aoede');
                        document.getElementById('audioStatus').textContent = 'Playing with browser TTS...';
                    }
                }

                // Auto-restart recording in continuous mode after response
                if (continuousMode && isRecording) {
                    setTimeout(() => {
                        if (continuousMode) {
                            document.getElementById('recordingStatus').textContent = 'VOICE Listening for your next question...';
                            restartListening();
                        }
                    }, 1500); // Wait 1.5 seconds after audio response
                }
            } else if (data.type === 'agent_switched') {
                // Handle automatic agent switching with delegation flow
                processDelegationFlow(data);
                updateCurrentAgent(data.new_agent, data.agent_name);
                addMessage(`[AGENT] Switched to ${data.agent_name}: ${data.message || 'Agent changed'}`, 'ai');
                document.getElementById('agentSelect').value = data.new_agent;

                // Add delegation log
                addDelegationLogEntry(` Agent switched to ${data.agent_name}`, 'delegation');
            } else if (data.type === 'delegation_announcement') {
                // Handle delegation announcement - show prominently in main chat
                addMessage(`[üì¢ DELEGATION] ${data.announcement}`, 'delegation');
                addA2AMessage(`üì¢ Delegation Announcement: ${data.announcement}`, 'ai', 'General Agent');
            } else if (data.type === 'a2a_delegation_confirmation') {
                // Handle A2A delegation confirmation - highly visible alert for user
                const message = `A2A ÏúÑÏûÑ ÌôïÏù∏: ${data.original_agent} ‚Üí ${data.specialist_agent}`;
                addMessage(`[A2A Ï†ÑÎ¨∏Í∞Ä ÏúÑÏûÑ] ${data.message}`, 'a2a_confirmation');

                // Show visual alert notification
                showA2AAlert(`A2A Ï†ÑÎ¨∏Í∞Ä ÏúÑÏûÑÏù¥ Î∞úÎèôÎêòÏóàÏäµÎãàÎã§!\nÏõêÎûò ÏóêÏù¥Ï†ÑÌä∏: ${data.original_agent}\nÏ†ÑÎ¨∏Í∞Ä ÏóêÏù¥Ï†ÑÌä∏: ${data.specialist_agent}\nÌä∏Î¶¨Í±∞: ${data.trigger}`);

                // Log delegation details
                console.log('A2A Delegation Details:', {
                    original: data.original_agent,
                    specialist: data.specialist_agent,
                    trigger: data.trigger,
                    response: data.specialist_response
                });
            } else if (data.type === 'a2a_conversation_event') {
                // Handle real-time A2A conversation events from the coordinator
                const eventData = data.event_data;
                const eventType = eventData.type;

                switch (eventType) {
                    case 'conversation_started':
                        addA2AMessage(` A2A Conversation Started: ${eventData.conversationId}`, 'system', 'Coordinator');
                        break;
                    case 'agent_turn_started':
                        const agentName = eventData.agent?.agentName || 'Unknown Agent';
                        addA2AMessage(`${agentName} is speaking...`, 'ai', agentName);
                        break;
                    case 'agent_turn_completed':
                        const completedAgent = eventData.agent?.agentName || 'Unknown Agent';
                        const response = eventData.response || 'No response';
                        addA2AMessage(` ${completedAgent}: ${response.substring(0, 100)}${response.length > 100 ? '...' : ''}`, 'ai', completedAgent);
                        break;
                    case 'conversation_continuation':
                        const nextAgent = eventData.nextAgent?.agentName || 'Unknown Agent';
                        addA2AMessage(` Switching to ${nextAgent}`, 'system', 'Coordinator');
                        break;
                    case 'conversation_error':
                        const error = eventData.error || 'Unknown error';
                        addA2AMessage(`‚ùå Error: ${error}`, 'system', 'Error');
                        break;
                    default:
                        addA2AMessage(`üìã Event: ${eventType}`, 'system', 'Coordinator');
                }
            } else if (data.type === 'a2a_delegation') {
                // Handle detailed A2A delegation information with conversation log
                const details = data.delegation_details;
                addA2AMessage(` A2A DELEGATION INITIATED`, 'system', 'Protocol');

                // Show delegation announcement if available
                if (details.announcement) {
                    addA2AMessage(`üì¢ ${details.announcement}`, 'ai', details.from_agent);
                }

                // Show conversation log if available
                if (details.conversation_log && details.conversation_log.length > 0) {
                    addA2AMessage(`üìñ === Agent Conversation Log ===`, 'system', 'Protocol');

                    details.conversation_log.forEach((log_entry, index) => {
                        const timestamp = new Date(log_entry.timestamp * 1000).toLocaleTimeString();
                        let icon = '';
                        let messageClass = '';

                        switch(log_entry.role) {
                            case 'user':
                                icon = 'üë§';
                                messageClass = 'user';
                                break;
                            case 'delegation':
                                icon = '';
                                messageClass = 'ai';
                                break;
                            case 'assistant':
                                icon = 'AGENT';
                                messageClass = 'ai';
                                break;
                            default:
                                icon = 'MESSAGE';
                                messageClass = 'ai';
                        }

                        addA2AMessage(`${icon} [${timestamp}] ${log_entry.message}`, messageClass, log_entry.agent);
                    });

                    addA2AMessage(`üìñ === End of Conversation Log ===`, 'system', 'Protocol');
                } else {
                    // Fallback to old format
                    addA2AMessage(`üìù User Request: "${details.user_request}"`, 'user', 'User');
                    addA2AMessage(`AGENT From: ${details.from_agent} toTo: ${details.to_agent}`, 'ai', 'Coordinator');
                    addA2AMessage(`üí° Reason: ${details.reason}`, 'ai', 'System');
                    addA2AMessage(`MESSAGE Specialist Response: "${details.specialist_response}"`, 'ai', details.to_agent);
                }
            } else if (data.type === 'agents_list') {
                // Handle agent list response
                updateAgentsList(data.agents, data.current_agent);
                addMessage(`[AGENT] Agent list updated. Current: ${data.current_agent}`, 'ai');
            } else if (data.type === 'agent_info') {
                // Handle agent info response
                updateAgentInfo(data);
            } else if (data.type === 'delegation_in_progress') {
                // Handle real-time delegation notification
                addMessage(`[] ${data.message}`, 'ai');
            } else if (data.type === 'voice_session_status') {
                // Handle continuous voice session status (from continuous_voice.html)
                if (data.status === 'started') {
                    startAudioCapture();
                }
                addMessage(data.message, 'ai');
            } else if (data.type === 'audio_chunk') {
                // Handle real-time audio chunks (from continuous_voice.html)
                if (data.audio && outputProcessor) {
                    const decoded = Uint8Array.from(
                        atob(data.audio), c => c.charCodeAt(0)
                    ).buffer;
                    outputProcessor.port.postMessage({'enqueue': decoded});
                }
            } else if (data.type === 'transcript') {
                // Handle AI transcript (from continuous_voice.html)
                if (data.text) {
                    addMessage(`AGENT AI: ${data.text}`, 'ai');
                }
            } else if (data.type === 'user_transcript') {
                // Handle user transcript (from continuous_voice.html) 
                if (data.text) {
                    addMessage(`üë§ You: ${data.text}`, 'user');
                }
            } else if (data.type === 'error') {
                addMessage(`[ERROR] ${data.message}`, 'ai');
            }
        };

        function addMessage(msg, sender) {
            const div = document.createElement('div');
            div.className = 'message ' + sender;
            div.innerHTML = msg;
            chat.appendChild(div);
            chat.scrollTop = chat.scrollHeight;
        }

        function showA2AAlert(message) {
            // Show highly visible alert for A2A delegation
            alert(`A2A ÏãúÏä§ÌÖú ÏïåÎ¶º\n\n${message}`);

            // Also log to console for debugging
            console.log(`%cA2A DELEGATION ALERT: ${message}`, 'color: #ff9800; font-weight: bold; font-size: 14px;');
        }

        function addA2AMessage(msg, sender, agentName = '') {
            const a2aChat = document.getElementById('a2aChat');
            const div = document.createElement('div');
            div.className = 'message ' + sender;

            // Add timestamp and agent info
            const timestamp = new Date().toLocaleTimeString();
            const agentInfo = agentName ? ` [${agentName}]` : '';
            div.innerHTML = `<small>${timestamp}${agentInfo}</small><br>${msg}`;

            a2aChat.appendChild(div);
            a2aChat.scrollTop = a2aChat.scrollHeight;
        }

        function sendMessage() {
            const input = document.getElementById('messageInput');
            const msg = input.value.trim();
            if (msg && ws.readyState === WebSocket.OPEN) {
                addMessage('[TXT] ' + msg, 'user');

                // Show current agent is thinking
                updateAgentStatus(currentActiveAgent, 'Thinking');
                addDelegationLogEntry(`üìù User message: "${msg.substring(0, 50)}${msg.length > 50 ? '...' : ''}"`, 'info');

                ws.send(JSON.stringify({type: 'text', message: msg}));
                input.value = '';
            } else if (ws.readyState !== WebSocket.OPEN) {
                alert('Not connected. Please refresh the page.');
            }
        }

        function uploadImage() {
            const file = document.getElementById('imageInput').files[0];
            const prompt = document.getElementById('imagePrompt').value || 'What do you see in this image?';

            if (file && ws.readyState === WebSocket.OPEN) {
                const reader = new FileReader();
                reader.onload = (e) => {
                    addMessage(`[IMG] Analyzing image: "${prompt}"`, 'user');

                    // Show image preview
                    const img = document.createElement('img');
                    img.src = e.target.result;
                    img.className = 'image-preview';
                    chat.appendChild(img);
                    chat.scrollTop = chat.scrollHeight;

                    ws.send(JSON.stringify({
                        type: 'image',
                        image: e.target.result,
                        prompt: prompt
                    }));
                };
                reader.readAsDataURL(file);
            } else if (!file) {
                alert('Please select an image file.');
            } else {
                alert('WebSocket not connected.');
            }
        }

        function sendAudioMessage() {
            const input = document.getElementById('audioInput');
            const voiceSelect = document.getElementById('voiceSelect');
            const msg = input.value.trim();

            // Auto-select voice based on current agent
            const voice = agentVoiceMap[currentActiveAgent] || voiceSelect.value;
            voiceSelect.value = voice; // Update the selector

            if (msg && ws.readyState === WebSocket.OPEN) {
                addMessage(`[üîä] ${msg} (Voice: ${voice} - Agent: ${currentActiveAgent})`, 'user');

                // Show current agent is thinking and log the message
                updateAgentStatus(currentActiveAgent, 'Thinking');
                addDelegationLogEntry(`üîä Audio message: "${msg.substring(0, 50)}${msg.length > 50 ? '...' : ''}"`, 'info');

                document.getElementById('audioStatus').textContent = 'Processing audio...';

                ws.send(JSON.stringify({
                    type: 'text_audio',
                    message: msg,
                    voice: voice
                }));

                input.value = '';
            } else if (ws.readyState !== WebSocket.OPEN) {
                alert('Not connected. Please refresh the page.');
            }
        }

        function playAudioResponse(audioBase64) {
            try {
                // Convert base64 to Uint8Array
                const audioBytes = atob(audioBase64);
                const audioArray = new Uint8Array(audioBytes.length);
                for (let i = 0; i < audioBytes.length; i++) {
                    audioArray[i] = audioBytes.charCodeAt(i);
                }

                // Convert PCM to WAV format
                const wavBlob = convertPCMToWAV(audioArray);
                const audioUrl = URL.createObjectURL(wavBlob);

                // Play audio
                const audioPlayer = document.getElementById('audioPlayer');
                audioPlayer.src = audioUrl;
                audioPlayer.style.display = 'block';
                audioPlayer.play();

                document.getElementById('audioStatus').textContent = 'Playing audio response...';

                // Clean up URL after playing
                audioPlayer.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    document.getElementById('audioStatus').textContent = '';
                };
            } catch (error) {
                console.error('Audio playback error:', error);
                document.getElementById('audioStatus').textContent = 'Audio playback error: ' + error.message;
            }
        }

        function convertPCMToWAV(pcmData) {
            // PCM to WAV conversion
            const sampleRate = 24000; // Gemini Audio uses 24kHz
            const numChannels = 1;
            const bytesPerSample = 2; // 16-bit PCM

            const buffer = new ArrayBuffer(44 + pcmData.length);
            const view = new DataView(buffer);

            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };

            writeString(0, 'RIFF');
            view.setUint32(4, 36 + pcmData.length, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true); // PCM format
            view.setUint16(20, 1, true); // Audio format
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * numChannels * bytesPerSample, true);
            view.setUint16(32, numChannels * bytesPerSample, true);
            view.setUint16(34, 16, true); // Bits per sample
            writeString(36, 'data');
            view.setUint32(40, pcmData.length, true);

            // Copy PCM data
            for (let i = 0; i < pcmData.length; i++) {
                view.setUint8(44 + i, pcmData[i]);
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

        function speakText(text, voiceName) {
            if ('speechSynthesis' in window) {
                // Cancel any ongoing speech
                window.speechSynthesis.cancel();

                const utterance = new SpeechSynthesisUtterance(text);

                // Voice mapping
                const voiceMap = {
                    'Aoede': 'female',
                    'Charon': 'male',
                    'Fenrir': 'male',
                    'Kore': 'female',
                    'Puck': 'male'
                };

                // Try to find a suitable voice
                const voices = window.speechSynthesis.getVoices();
                const preferredGender = voiceMap[voiceName] || 'female';

                const suitableVoice = voices.find(voice =>
                    voice.lang.startsWith('en') &&
                    voice.name.toLowerCase().includes(preferredGender === 'female' ? 'female' : 'male')
                ) || voices.find(voice => voice.lang.startsWith('en'));

                if (suitableVoice) {
                    utterance.voice = suitableVoice;
                }

                utterance.rate = 1.0;
                utterance.pitch = 1.0;
                utterance.volume = 1.0;

                utterance.onend = () => {
                    document.getElementById('audioStatus').textContent = '';
                };

                utterance.onerror = (error) => {
                    console.error('Speech synthesis error:', error);
                    document.getElementById('audioStatus').textContent = 'TTS error';
                };

                window.speechSynthesis.speak(utterance);
            } else {
                console.warn('Speech synthesis not supported');
                document.getElementById('audioStatus').textContent = 'TTS not supported';
            }
        }

        function clearChat() {
            chat.innerHTML = '';
            addMessage('Chat cleared. Ready for new conversation!', 'ai');
        }

        // Audio recording variables for Live API streaming
        let mediaRecorder;
        let audioStream;
        let isRecording = false;
        let continuousMode = false;
        let voiceSessionActive = false;
        let processor;
        let recognition = null;
        let silenceTimer = null;
        let silenceCount = 0;
        let lastProcessedTranscript = '';

        async function startRecording() {
            const audioStatus = document.getElementById('recordingStatus');


            if (isRecording) {
                return; // Already recording
            }

            try {
                // Browser and security checks
                if (location.protocol !== 'https:' && location.hostname !== 'localhost' && location.hostname !== '127.0.0.1') {
                    audioStatus.textContent = '‚ö†Ô∏è Microphone requires HTTPS or localhost';
                    alert('Microphone access requires HTTPS or localhost.');
                    return;
                }

                if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                    audioStatus.textContent = '‚ö†Ô∏è Browser does not support microphone access';
                    alert('Your browser does not support microphone access.');
                    return;
                }

                // Voice session handled via WebSocket

                // Get microphone access with optimal settings for Gemini Live API
                audioStream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        channelCount: 1,
                        sampleRate: 24000,  // Gemini Live API preferred rate
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true
                    }
                });

                // Setup audio processing for real-time streaming
                audioContext = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 24000
                });

                const source = audioContext.createMediaStreamSource(audioStream);

                // Create script processor for real-time audio chunks
                processor = audioContext.createScriptProcessor(4096, 1, 1);

                processor.onaudioprocess = function(event) {
                    if (isRecording && voiceSessionActive) {
                        const inputBuffer = event.inputBuffer;
                        const inputData = inputBuffer.getChannelData(0);

                        // Convert Float32Array to Int16Array (PCM format for Gemini)
                        const pcmData = new Int16Array(inputData.length);
                        for (let i = 0; i < inputData.length; i++) {
                            pcmData[i] = Math.max(-32768, Math.min(32767, inputData[i] * 32768));
                        }

                        // Send PCM audio chunk to backend via WebSocket
                        if (ws && ws.readyState === WebSocket.OPEN) {
                            const encoded = btoa(String.fromCharCode(...new Uint8Array(pcmData.buffer)));
                            ws.send(JSON.stringify({
                                type: 'voice_audio_chunk',
                                audio: encoded
                            }));
                        }
                    }
                };

                source.connect(processor);
                processor.connect(audioContext.destination);

                isRecording = true;
                audioStatus.textContent = continuousMode ?
                    'VOICE Ïã§ÏãúÍ∞Ñ ÎåÄÌôî Ï§ë... ÏûêÏó∞Ïä§ÎüΩÍ≤å ÎßêÏîÄÌïòÏÑ∏Ïöî' :
                    'VOICE Recording... Speak now';

                console.log('Direct audio streaming to Gemini Live API started');

                // Setup speech recognition for voice commands
                if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                    recognition = new SpeechRecognition();
                    recognition.continuous = continuousMode;
                    recognition.interimResults = true;
                    recognition.lang = 'ko-KR';

                    recognition.onresult = (event) => {
                        const result = event.results[event.results.length - 1];
                        const transcript = result[0].transcript.trim();
                        const confidence = result[0].confidence || 0.8;

                        // Clear silence timer on speech detection
                        if (silenceTimer) {
                            clearTimeout(silenceTimer);
                            silenceTimer = null;
                        }

                        // Process on final result or high confidence partial
                        if (result.isFinal || (transcript.length > 2 && confidence > 0.75)) {
                            if (transcript !== lastProcessedTranscript && transcript && ws.readyState === WebSocket.OPEN) {
                                lastProcessedTranscript = transcript;

                                addMessage(`VOICE "${transcript}"`, 'user');
                                audioStatus.textContent = 'Processing with A2A agents...';

                                // Fast A2A processing
                                ws.send(JSON.stringify({
                                    type: 'text_audio',
                                    message: transcript,
                                    voice: 'Aoede',
                                    timestamp: performance.now(),
                                    continuous_mode: continuousMode
                                }));

                                // Don't stop if in continuous mode
                                if (!continuousMode) {
                                    stopRecording();
                                } else {
                                    audioStatus.textContent = 'Waiting for AI response...';
                                }
                            }
                        } else if (transcript.length > 0) {
                            audioStatus.textContent = `VOICE "${transcript}..." (listening)`;
                        }

                        // Auto-stop after silence in continuous mode
                        if (continuousMode && result.isFinal) {
                            silenceCount++;
                            silenceTimer = setTimeout(() => {
                                if (silenceCount > 2) { // After 2 final results with no new speech
                                    audioStatus.textContent = 'Listening for your next question...';
                                    silenceCount = 0;
                                }
                            }, 2000);
                        }
                    };

                    recognition.onerror = (event) => {
                        if (event.error !== 'no-speech' && event.error !== 'aborted') {
                            audioStatus.textContent = 'Speech error: ' + event.error;
                        }
                    };

                    recognition.onstart = () => {
                        const mode = continuousMode ? 'continuous conversation' : 'single question';
                        audioStatus.textContent = `VOICE Listening (${mode})... Speak now!`;
                    };

                    recognition.onend = () => {
                        if (isRecording && recognition && continuousMode) {
                            setTimeout(() => {
                                if (continuousMode && isRecording) {
                                    try {
                                        recognition.start();
                                    } catch (e) {
                                        console.error('Failed to restart recognition:', e);
                                    }
                                }
                            }, 100);
                        }
                    };

                    recognition.start();
                }

            } catch (error) {
                    console.error('Error accessing microphone:', error);
                    audioStatus.textContent = 'Microphone access denied: ' + error.message;
                }
        }

        async function toggleAudio() {
            const audioButton = document.getElementById('audioButton');

            if (!isRecording) {
                await startRecording();
            } else {
                stopRecording();
            }
        }

        function stopRecording() {
            const audioButton = document.getElementById('audioButton');
            const audioStatus = document.getElementById('recordingStatus');

            console.log('stopRecording called');

            if (isRecording) {
                // Stop speech recognition
                if (recognition) {
                    console.log('Stopping speech recognition...');
                    recognition.stop();
                    recognition = null;
                }

                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    mediaRecorder.stop();
                }

                if (mediaRecorder && mediaRecorder.stream) {
                    mediaRecorder.stream.getTracks().forEach(track => {
                        console.log('Stopping track:', track);
                        track.stop();
                    });
                }

                isRecording = false;
                audioButton.textContent = 'VOICE Start Recording';
                audioButton.style.background = '#4CAF50';
                audioStatus.textContent = 'Ready to record';
                console.log('Recording stopped');
            }
        }

        // A2A Agent Management Functions
        function switchAgent() {
            const agentSelect = document.getElementById('agentSelect');
            const selectedAgent = agentSelect.value;

            if (selectedAgent && ws.readyState === WebSocket.OPEN) {
                addMessage(`[AGENT] Switching to agent: ${selectedAgent}`, 'user');

                ws.send(JSON.stringify({
                    type: 'switch_agent',
                    agent_slug: selectedAgent
                }));

                document.getElementById('agentStatus').textContent = 'Switching agent...';
            } else if (ws.readyState !== WebSocket.OPEN) {
                alert('Not connected. Please refresh the page.');
            }
        }

        function requestAgentList() {
            if (ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                    type: 'list_agents'
                }));

                document.getElementById('agentStatus').textContent = 'Refreshing agent list...';
            } else {
                console.warn('WebSocket not connected, cannot request agent list');
            }
        }

        function updateCurrentAgent(agentSlug, agentName) {
            const agentStatusEl = document.getElementById('agentStatus');
            const agentInfoEl = document.getElementById('agentInfo');

            agentStatusEl.textContent = `Current: ${agentName}`;

            // Update agent info with voice information
            const voiceInfo = agentVoiceMap[agentSlug] || 'Aoede';
            const agentConfigs = {
                'test-agent': `Voice: ${voiceInfo} (Male) - Confident coordinator voice for delegation`,
                'flight-specialist': `Voice: ${voiceInfo} (Female) - Professional travel specialist`,
                'hotel-specialist': `Voice: ${voiceInfo} (Female) - Friendly hospitality specialist`,
                'general-worker': `Voice: ${voiceInfo} (Female) - Warm and helpful general assistant`
            };

            agentInfoEl.textContent = agentConfigs[agentSlug] || `Voice: ${voiceInfo} - AI Assistant`;

            // Update dropdown selection
            const agentSelect = document.getElementById('agentSelect');
            agentSelect.value = agentSlug;

            // Update the delegation display
            updateCurrentAgentDisplay(agentSlug, agentName);
        }

        function updateAgentsList(agents, currentAgent) {
            const agentSelect = document.getElementById('agentSelect');

            // Clear existing options
            agentSelect.innerHTML = '';

            // Add agents to dropdown
            for (const [slug, info] of Object.entries(agents)) {
                const option = document.createElement('option');
                option.value = slug;
                option.textContent = `${info.name} (${info.voice_style})`;
                agentSelect.appendChild(option);
            }

            // Set current agent
            agentSelect.value = currentAgent;
            updateCurrentAgent(currentAgent, agents[currentAgent]?.name || 'Unknown Agent');
        }

        function updateAgentInfo(agentData) {
            const agentInfoEl = document.getElementById('agentInfo');
            agentInfoEl.textContent = `${agentData.agent_description} | Capabilities: ${agentData.capabilities?.join(', ') || 'Standard'}`;
        }

        // Start audio capture using Context7 SEPARATED worklet pattern (from continuous_voice.html)
        async function startAudioCapture() {
            try {
                // Set voice mode active to prevent TTS interference
                isVoiceActive = true;
                // Official Gemini Live API audio configuration
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        sampleRate: 16000,  // Official Gemini Live API input standard
                        echoCancellation: true,
                        noiseSuppression: true,
                        channelCount: 1
                    }
                });

                // Input: 16kHz, Output: 24kHz (per official Gemini cookbook)
                audioContext = new AudioContext({sampleRate: 24000});

                // Setup analyser for visualization
                analyser = audioContext.createAnalyser();
                microphone = audioContext.createMediaStreamSource(stream);
                microphone.connect(analyser);

                // Context7 pattern: Load separated AudioWorklet modules
                await audioContext.audioWorklet.addModule(URL.createObjectURL(
                    new Blob([inputWorkletCode], {type: 'text/javascript'})
                ));
                await audioContext.audioWorklet.addModule(URL.createObjectURL(
                    new Blob([outputWorkletCode], {type: 'text/javascript'})
                ));

                // Create SEPARATED processors
                inputProcessor = new AudioWorkletNode(audioContext, 'input-processor');
                outputProcessor = new AudioWorkletNode(audioContext, 'output-processor');

                // Real-time 12.5ms batch audio transmission
                inputProcessor.port.onmessage = (event) => {
                    if ('audio_in' in event.data && ws && ws.readyState === WebSocket.OPEN) {
                        // Context7 base64 encoding
                        const encoded = btoa(String.fromCharCode(
                            ...Array.from(new Uint8Array(event.data.audio_in))
                        ));

                        ws.send(JSON.stringify({
                            type: 'voice_audio_chunk',
                            audio: encoded
                        }));
                    }
                };

                // Connect SEPARATED audio pipeline (Context7 pattern)
                microphone.connect(inputProcessor);
                // INPUT: microphone -> inputProcessor (NO destination!)
                // OUTPUT: outputProcessor -> destination (separate path!)
                outputProcessor.connect(audioContext.destination);

                // Start visualization (from continuous_voice.html)
                visualizeAudio();

                addMessage('Official Gemini Live API (16kHz input/24kHz output) active with 50ms Context7 streaming - speak freely!', 'ai');

            } catch (error) {
                console.error('Error accessing microphone:', error);
                addMessage('Failed to access microphone', 'ai');
                stopConversation();
            }
        }

        // One-click continuous conversation (Enhanced with real-time streaming)
        async function startContinuousConversation() {
            const audioButton = document.getElementById('audioButton');
            const stopButton = document.getElementById('stopButton');
            const audioStatus = document.getElementById('recordingStatus');

            if (isRecording) {
                return; // Already recording
            }

            // Check WebSocket connection
            if (!ws || ws.readyState !== WebSocket.OPEN) {
                audioStatus.textContent = 'WebSocket not connected. Please refresh the page.';
                return;
            }

            // Enable continuous mode automatically
            continuousMode = true;
            isRecording = true;

            // Update UI
            audioButton.style.display = 'none';
            stopButton.style.display = 'inline-block';
            audioStatus.textContent = 'Starting real-time voice session...';

            // Start continuous voice session (from continuous_voice.html pattern)
            ws.send(JSON.stringify({
                type: 'start_voice_session'
            }));
        }

        function stopConversation() {
            const audioButton = document.getElementById('audioButton');
            const stopButton = document.getElementById('stopButton');
            const audioStatus = document.getElementById('recordingStatus');

            // Disable continuous mode
            continuousMode = false;
            isRecording = false;
            isVoiceActive = false;  // Re-enable TTS system

            // Clear SEPARATED audio worklets (from continuous_voice.html)
            if (inputProcessor) {
                inputProcessor.disconnect();
                inputProcessor = null;
            }
            if (outputProcessor) {
                outputProcessor.port.postMessage({'clear': ''});
                outputProcessor.disconnect();
                outputProcessor = null;
            }

            // Close audio context
            if (audioContext) {
                audioContext.close();
                audioContext = null;
            }

            // Send stop command to backend
            if (ws && ws.readyState === WebSocket.OPEN) {
                ws.send(JSON.stringify({
                    type: 'stop_voice_session'
                }));
            }

            // Stop old recording if exists
            stopRecording();

            // Update UI
            audioButton.style.display = 'inline-block';
            stopButton.style.display = 'none';
            audioStatus.textContent = 'Click to start continuous voice conversation';
            
            addMessage('Real-time conversation ended', 'ai');
        }

        // Visualize audio (from continuous_voice.html)
        function visualizeAudio() {
            if (!analyser || (!isRecording && !isVoiceActive)) return;

            const canvas = document.getElementById('visualizer');
            const ctx = canvas.getContext('2d');
            const bufferLength = analyser.frequencyBinCount;
            const dataArray = new Uint8Array(bufferLength);

            function draw() {
                if (!isRecording && !isVoiceActive) return;

                requestAnimationFrame(draw);

                analyser.getByteFrequencyData(dataArray);

                ctx.fillStyle = 'rgb(0, 0, 0)';
                ctx.fillRect(0, 0, canvas.width, canvas.height);

                const barWidth = (canvas.width / bufferLength) * 2.5;
                let barHeight;
                let x = 0;

                for (let i = 0; i < bufferLength; i++) {
                    barHeight = dataArray[i] / 2;

                    ctx.fillStyle = `rgb(${barHeight + 100}, 50, ${250 - barHeight})`;
                    ctx.fillRect(x, canvas.height - barHeight, barWidth, barHeight);

                    x += barWidth + 1;
                }
            }

            canvas.width = canvas.offsetWidth;
            canvas.height = canvas.offsetHeight;
            draw();
        }

        function restartListening() {
            if (recognition && continuousMode && isRecording) {
                try {
                    recognition.start();
                } catch (e) {
                    // If already running, don't worry
                    if (e.name !== 'InvalidStateError') {
                        console.error('Failed to restart recognition:', e);
                    }
                }
            }
        }

        // Optimized stopRecording function
        function stopRecording() {
            const audioButton = document.getElementById('audioButton');
            const stopButton = document.getElementById('stopButton');
            const audioStatus = document.getElementById('recordingStatus');

            console.log('stopRecording called');

            if (isRecording) {
                // Clear any timers
                if (silenceTimer) {
                    clearTimeout(silenceTimer);
                    silenceTimer = null;
                }

                // Stop speech recognition
                if (recognition) {
                    recognition.stop();
                    recognition = null;
                }

                // Stop media recorder
                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    mediaRecorder.stop();
                }

                // Stop all tracks
                if (mediaRecorder && mediaRecorder.stream) {
                    mediaRecorder.stream.getTracks().forEach(track => track.stop());
                }

                isRecording = false;

                // Reset UI to default state
                audioButton.textContent = 'VOICE Start Real-time Conversation';
                audioButton.style.background = '#667eea';
                audioButton.style.display = 'inline-block';
                stopButton.style.display = 'none';
                audioStatus.textContent = 'Click to start continuous voice conversation';
            }
        }
    </script>
</body>
</html>