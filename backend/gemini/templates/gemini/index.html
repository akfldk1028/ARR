<!DOCTYPE html>
<html>
<head>
    <title>{{ page_title }}</title>
    <meta charset="UTF-8">
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
        }
        .container {
            background: white;
            border-radius: 20px;
            padding: 30px;
            box-shadow: 0 20px 60px rgba(0,0,0,0.3);
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
        }
        .header h1 {
            color: #333;
            margin-bottom: 10px;
        }
        .header p {
            color: #666;
            font-size: 14px;
        }
        .chat {
            border: 2px solid #e0e0e0;
            height: 500px;
            overflow-y: auto;
            padding: 20px;
            margin: 20px 0;
            background: #f9f9f9;
            border-radius: 15px;
        }
        .message {
            margin: 15px 0;
            padding: 15px;
            border-radius: 10px;
            max-width: 80%;
            animation: fadeIn 0.3s;
        }
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }
        .user {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            margin-left: auto;
            text-align: right;
        }
        .ai {
            background: white;
            border: 1px solid #e0e0e0;
            margin-right: auto;
        }
        .controls {
            margin: 20px 0;
            padding: 20px;
            background: #f5f5f5;
            border-radius: 15px;
        }
        .control-group {
            margin-bottom: 20px;
        }
        .control-group h3 {
            color: #333;
            margin-bottom: 10px;
        }
        input, button {
            padding: 12px 20px;
            margin: 5px;
            border: 2px solid #e0e0e0;
            border-radius: 8px;
            font-size: 14px;
            transition: all 0.3s;
        }
        button {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            border: none;
            cursor: pointer;
            font-weight: bold;
        }
        button:hover {
            transform: translateY(-2px);
            box-shadow: 0 5px 15px rgba(102, 126, 234, 0.4);
        }
        #messageInput { width: 60%; }
        #imagePrompt { width: 50%; }
        .status {
            padding: 10px 20px;
            margin: 10px 0;
            border-radius: 8px;
            text-align: center;
            font-weight: bold;
            transition: all 0.3s;
        }
        .connected {
            background: #d4edda;
            color: #155724;
            border: 1px solid #c3e6cb;
        }
        .disconnected {
            background: #f8d7da;
            color: #721c24;
            border: 1px solid #f5c6cb;
        }
        .image-preview {
            max-width: 300px;
            margin: 10px 0;
            border-radius: 10px;
            box-shadow: 0 5px 15px rgba(0,0,0,0.2);
        }
        .info-badge {
            display: inline-block;
            padding: 4px 8px;
            background: #f0f0f0;
            border-radius: 4px;
            font-size: 12px;
            color: #666;
            margin-left: 10px;
        }
        .capabilities {
            display: flex;
            gap: 10px;
            justify-content: center;
            margin-top: 10px;
        }
        .capability {
            padding: 5px 15px;
            background: #e3f2fd;
            color: #1976d2;
            border-radius: 20px;
            font-size: 12px;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>{{ page_title }}</h1>
            <p>Model: {{ model_name }} | Django + Channels WebSocket</p>
            <div class="capabilities">
                <span class="capability">Text</span>
                <span class="capability">Image</span>
                <span class="capability">Video (Soon)</span>
                <span class="capability">Audio (Input)</span>
            </div>
        </div>

        <div id="status" class="status disconnected">Connecting to Live API...</div>

        <div id="chat" class="chat"></div>

        <div class="controls">
            <div class="control-group">
                <h3>Image Analysis</h3>
                <input type="file" id="imageInput" accept="image/*">
                <input type="text" id="imagePrompt" placeholder="Ask about the image..." value="What do you see in this image?">
                <button onclick="uploadImage()">Analyze Image</button>
            </div>

            <div class="control-group">
                <h3>Audio Chat (TTS)</h3>
                <input type="text" id="audioInput" placeholder="Type message for voice response..." style="width: 50%;">
                <select id="voiceSelect" style="margin: 5px;">
                    <option value="Aoede">Aoede (Default)</option>
                    <option value="Charon">Charon</option>
                    <option value="Fenrir">Fenrir</option>
                    <option value="Kore">Kore</option>
                    <option value="Puck">Puck</option>
                </select>
                <button onclick="sendAudioMessage()">🔊 Get Voice Response</button>
                <div id="audioStatus" style="margin: 10px 0; font-size: 12px; color: #666;"></div>
                <audio id="audioPlayer" controls style="display: none; margin: 10px 0;"></audio>
            </div>

            <div class="control-group">
                <h3>Audio Recording</h3>
                <button id="audioButton" onclick="toggleAudio()">🎤 Start Recording</button>
                <div id="recordingStatus" style="margin: 10px 0; font-size: 12px; color: #666;"></div>
            </div>

            <div class="control-group">
                <h3>Text Chat</h3>
                <input type="text" id="messageInput" placeholder="Type your message..." onkeypress="if(event.key==='Enter') sendMessage()">
                <button onclick="sendMessage()">Send</button>
                <button onclick="clearChat()">Clear</button>
            </div>
        </div>
    </div>

    <script>
        console.log('Starting WebSocket connection to:', '{{ websocket_url }}');
        const ws = new WebSocket('{{ websocket_url }}');
        const chat = document.getElementById('chat');
        const status = document.getElementById('status');

        ws.onopen = () => {
            console.log('WebSocket connected!');
            status.textContent = 'Connected to Django + Gemini Live API';
            status.className = 'status connected';
        };

        ws.onclose = (event) => {
            console.log('WebSocket closed:', event.code, event.reason);
            status.textContent = 'Disconnected';
            status.className = 'status disconnected';
        };

        ws.onerror = (error) => {
            console.error('WebSocket error details:', error);
            status.textContent = 'Connection Error';
            status.className = 'status disconnected';
        };

        // Streaming variables
        let currentStreamingMessage = null;
        let streamingMessageElement = null;

        ws.onmessage = (event) => {
            const data = JSON.parse(event.data);
            console.log('WebSocket received:', data); // DEBUG

            if (data.type === 'connection') {
                addMessage(`Connected! Model: ${data.model}`, 'ai');
            } else if (data.type === 'streaming_chunk') {
                // Handle real-time streaming chunks
                if (!currentStreamingMessage) {
                    // Create new streaming message
                    currentStreamingMessage = '';
                    streamingMessageElement = document.createElement('div');
                    streamingMessageElement.className = 'message ai';
                    streamingMessageElement.innerHTML = '[STREAM] ';
                    chat.appendChild(streamingMessageElement);
                }

                if (data.chunk) {
                    currentStreamingMessage += data.chunk;
                    streamingMessageElement.innerHTML = `[STREAM] ${currentStreamingMessage}`;
                    chat.scrollTop = chat.scrollHeight;
                }

                if (data.is_final) {
                    // Finalize streaming message
                    streamingMessageElement.innerHTML = `[TXT] ${currentStreamingMessage}`;
                    currentStreamingMessage = null;
                    streamingMessageElement = null;
                }
            } else if (data.type === 'response' || data.type === 'image_response') {
                const time = data.response_time ? ` <span class="info-badge">${data.response_time.toFixed(2)}s</span>` : '';
                let typeIcon = '[TXT]';
                if (data.type === 'image_response') typeIcon = '[IMG]';
                else if (data.analysis_type === 'image_regular_api') typeIcon = '[IMG]';
                else if (data.analysis_type === 'audio_live_api') typeIcon = '[AUD]';
                else if (data.analysis_type === 'video_frame_analysis') typeIcon = '[VID]';
                addMessage(`${typeIcon} ${data.message}${time}`, 'ai');
            } else if (data.type === 'audio_response') {
                const time = data.response_time ? ` <span class="info-badge">${data.response_time.toFixed(2)}s</span>` : '';
                const transcript = data.transcript || 'No response generated';
                addMessage(`[🔊] ${transcript}${time} (Voice: ${data.voice})`, 'ai');

                // Play audio if available, otherwise use browser TTS
                if (data.audio && data.success) {
                    playAudioResponse(data.audio);
                } else if (transcript && transcript !== 'No response generated') {
                    // Fallback to browser TTS
                    speakText(transcript, data.voice || 'Aoede');
                    document.getElementById('audioStatus').textContent = 'Playing with browser TTS...';
                }
            } else if (data.type === 'error') {
                addMessage(`[ERROR] ${data.message}`, 'ai');
            }
        };

        function addMessage(msg, sender) {
            const div = document.createElement('div');
            div.className = 'message ' + sender;
            div.innerHTML = msg;
            chat.appendChild(div);
            chat.scrollTop = chat.scrollHeight;
        }

        function sendMessage() {
            const input = document.getElementById('messageInput');
            const msg = input.value.trim();
            if (msg && ws.readyState === WebSocket.OPEN) {
                addMessage('[TXT] ' + msg, 'user');
                ws.send(JSON.stringify({type: 'text', message: msg}));
                input.value = '';
            } else if (ws.readyState !== WebSocket.OPEN) {
                alert('Not connected. Please refresh the page.');
            }
        }

        function uploadImage() {
            const file = document.getElementById('imageInput').files[0];
            const prompt = document.getElementById('imagePrompt').value || 'What do you see in this image?';

            if (file && ws.readyState === WebSocket.OPEN) {
                const reader = new FileReader();
                reader.onload = (e) => {
                    addMessage(`[IMG] Analyzing image: "${prompt}"`, 'user');

                    // Show image preview
                    const img = document.createElement('img');
                    img.src = e.target.result;
                    img.className = 'image-preview';
                    chat.appendChild(img);
                    chat.scrollTop = chat.scrollHeight;

                    ws.send(JSON.stringify({
                        type: 'image',
                        image: e.target.result,
                        prompt: prompt
                    }));
                };
                reader.readAsDataURL(file);
            } else if (!file) {
                alert('Please select an image file.');
            } else {
                alert('WebSocket not connected.');
            }
        }

        function sendAudioMessage() {
            const input = document.getElementById('audioInput');
            const voiceSelect = document.getElementById('voiceSelect');
            const msg = input.value.trim();
            const voice = voiceSelect.value;

            if (msg && ws.readyState === WebSocket.OPEN) {
                addMessage(`[🔊] ${msg} (Voice: ${voice})`, 'user');

                document.getElementById('audioStatus').textContent = 'Processing audio...';

                ws.send(JSON.stringify({
                    type: 'text_audio',
                    message: msg,
                    voice: voice
                }));

                input.value = '';
            } else if (ws.readyState !== WebSocket.OPEN) {
                alert('Not connected. Please refresh the page.');
            }
        }

        function playAudioResponse(audioBase64) {
            try {
                // Convert base64 to Uint8Array
                const audioBytes = atob(audioBase64);
                const audioArray = new Uint8Array(audioBytes.length);
                for (let i = 0; i < audioBytes.length; i++) {
                    audioArray[i] = audioBytes.charCodeAt(i);
                }

                // Convert PCM to WAV format
                const wavBlob = convertPCMToWAV(audioArray);
                const audioUrl = URL.createObjectURL(wavBlob);

                // Play audio
                const audioPlayer = document.getElementById('audioPlayer');
                audioPlayer.src = audioUrl;
                audioPlayer.style.display = 'block';
                audioPlayer.play();

                document.getElementById('audioStatus').textContent = 'Playing audio response...';

                // Clean up URL after playing
                audioPlayer.onended = () => {
                    URL.revokeObjectURL(audioUrl);
                    document.getElementById('audioStatus').textContent = '';
                };
            } catch (error) {
                console.error('Audio playback error:', error);
                document.getElementById('audioStatus').textContent = 'Audio playback error: ' + error.message;
            }
        }

        function convertPCMToWAV(pcmData) {
            // PCM to WAV conversion
            const sampleRate = 24000; // Gemini Audio uses 24kHz
            const numChannels = 1;
            const bytesPerSample = 2; // 16-bit PCM

            const buffer = new ArrayBuffer(44 + pcmData.length);
            const view = new DataView(buffer);

            // WAV header
            const writeString = (offset, string) => {
                for (let i = 0; i < string.length; i++) {
                    view.setUint8(offset + i, string.charCodeAt(i));
                }
            };

            writeString(0, 'RIFF');
            view.setUint32(4, 36 + pcmData.length, true);
            writeString(8, 'WAVE');
            writeString(12, 'fmt ');
            view.setUint32(16, 16, true); // PCM format
            view.setUint16(20, 1, true); // Audio format
            view.setUint16(22, numChannels, true);
            view.setUint32(24, sampleRate, true);
            view.setUint32(28, sampleRate * numChannels * bytesPerSample, true);
            view.setUint16(32, numChannels * bytesPerSample, true);
            view.setUint16(34, 16, true); // Bits per sample
            writeString(36, 'data');
            view.setUint32(40, pcmData.length, true);

            // Copy PCM data
            for (let i = 0; i < pcmData.length; i++) {
                view.setUint8(44 + i, pcmData[i]);
            }

            return new Blob([buffer], { type: 'audio/wav' });
        }

        function speakText(text, voiceName) {
            if ('speechSynthesis' in window) {
                // Cancel any ongoing speech
                window.speechSynthesis.cancel();

                const utterance = new SpeechSynthesisUtterance(text);

                // Voice mapping
                const voiceMap = {
                    'Aoede': 'female',
                    'Charon': 'male',
                    'Fenrir': 'male',
                    'Kore': 'female',
                    'Puck': 'male'
                };

                // Try to find a suitable voice
                const voices = window.speechSynthesis.getVoices();
                const preferredGender = voiceMap[voiceName] || 'female';

                const suitableVoice = voices.find(voice =>
                    voice.lang.startsWith('en') &&
                    voice.name.toLowerCase().includes(preferredGender === 'female' ? 'female' : 'male')
                ) || voices.find(voice => voice.lang.startsWith('en'));

                if (suitableVoice) {
                    utterance.voice = suitableVoice;
                }

                utterance.rate = 1.0;
                utterance.pitch = 1.0;
                utterance.volume = 1.0;

                utterance.onend = () => {
                    document.getElementById('audioStatus').textContent = '';
                };

                utterance.onerror = (error) => {
                    console.error('Speech synthesis error:', error);
                    document.getElementById('audioStatus').textContent = 'TTS error';
                };

                window.speechSynthesis.speak(utterance);
            } else {
                console.warn('Speech synthesis not supported');
                document.getElementById('audioStatus').textContent = 'TTS not supported';
            }
        }

        function clearChat() {
            chat.innerHTML = '';
            addMessage('Chat cleared. Ready for new conversation!', 'ai');
        }

        // Audio recording variables
        let mediaRecorder;
        let audioChunks = [];
        let isRecording = false;

        // Speech recognition global variable
        let recognition = null;

        async function toggleAudio() {
            const audioButton = document.getElementById('audioButton');
            const audioStatus = document.getElementById('recordingStatus');

            console.log('toggleAudio called, isRecording:', isRecording);

            if (!isRecording) {
                try {
                    // Check if we're on HTTPS or localhost (required for getUserMedia)
                    if (location.protocol !== 'https:' && location.hostname !== 'localhost' && location.hostname !== '127.0.0.1') {
                        audioStatus.textContent = '⚠️ Microphone requires HTTPS or localhost';
                        console.error('Microphone access requires HTTPS or localhost');
                        alert('Microphone access requires HTTPS or localhost. Please use https:// or access from localhost.');
                        return;
                    }

                    // Check browser compatibility
                    if (!navigator.mediaDevices || !navigator.mediaDevices.getUserMedia) {
                        audioStatus.textContent = '⚠️ Browser does not support microphone access';
                        console.error('getUserMedia not supported');
                        alert('Your browser does not support microphone access. Please use Chrome, Firefox, or Edge.');
                        return;
                    }

                    // Start Web Speech API first
                    if ('webkitSpeechRecognition' in window || 'SpeechRecognition' in window) {
                        console.log('Initializing Web Speech API...');
                        const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
                        recognition = new SpeechRecognition();

                        recognition.continuous = true;
                        recognition.interimResults = true; // Enable partial results for faster response
                        recognition.lang = 'ko-KR';  // Korean language recognition
                        recognition.maxAlternatives = 1; // Reduce processing overhead

                        let lastProcessedTranscript = '';
                        recognition.onresult = (event) => {
                            const result = event.results[event.results.length - 1];
                            const transcript = result[0].transcript.trim();

                            // Process immediately on final result or confident partial result
                            if (result.isFinal || (transcript.length > 3 && result[0].confidence > 0.8)) {
                                console.log('Speech recognized:', transcript, 'Final:', result.isFinal, 'Confidence:', result[0].confidence);

                                // Avoid duplicate processing
                                if (transcript !== lastProcessedTranscript && transcript && ws.readyState === WebSocket.OPEN) {
                                    lastProcessedTranscript = transcript;

                                    addMessage(`🎤 "${transcript}"`, 'user');
                                    audioStatus.textContent = 'Getting AI response...';

                                    // Send immediately for faster response
                                    const startTime = performance.now();
                                    ws.send(JSON.stringify({
                                        type: 'text_audio',
                                        message: transcript,
                                        voice: 'Aoede',
                                        timestamp: startTime
                                    }));

                                    // Stop after processing
                                    stopRecording();
                                }
                            } else if (transcript.length > 0) {
                                // Show interim results for user feedback
                                audioStatus.textContent = `🎤 "${transcript}..." (listening)`;
                            }
                        };

                        recognition.onerror = (event) => {
                            console.error('Speech recognition error:', event.error);
                            if (event.error !== 'no-speech' && event.error !== 'aborted') {
                                audioStatus.textContent = 'Speech error: ' + event.error;
                            }
                        };

                        recognition.onstart = () => {
                            console.log('Speech recognition started');
                            audioStatus.textContent = '🎤 Listening... Speak now!';
                        };

                        recognition.onend = () => {
                            console.log('Speech recognition ended');
                            if (isRecording && recognition) {
                                // Restart if still recording
                                try {
                                    recognition.start();
                                } catch (e) {
                                    console.error('Failed to restart recognition:', e);
                                }
                            }
                        };

                        // Start speech recognition
                        recognition.start();
                    }

                    console.log('Requesting microphone access...');
                    audioStatus.textContent = '📡 Requesting microphone permission...';

                    // Request with more specific constraints
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                            sampleRate: 44100
                        }
                    });
                    console.log('Microphone access granted, stream:', stream);

                    mediaRecorder = new MediaRecorder(stream);
                    audioChunks = [];
                    console.log('MediaRecorder created:', mediaRecorder);

                    mediaRecorder.ondataavailable = (event) => {
                        console.log('Audio data available:', event.data.size, 'bytes');
                        audioChunks.push(event.data);
                    };

                    console.log('Starting recording...');
                    mediaRecorder.start();
                    isRecording = true;
                    audioButton.textContent = '⏹️ Stop Recording';
                    audioButton.style.background = '#dc3545';
                    addMessage('🎤 Recording started... Speak now!', 'user');
                    console.log('Recording started successfully');
                } catch (error) {
                    console.error('Error accessing microphone:', error);
                    audioStatus.textContent = 'Microphone access denied: ' + error.message;
                }
            } else {
                console.log('Stopping recording...');
                // Stop speech recognition
                if (recognition) {
                    console.log('Stopping speech recognition...');
                    recognition.stop();
                    recognition = null;
                }

                mediaRecorder.stop();
                mediaRecorder.stream.getTracks().forEach(track => {
                    console.log('Stopping track:', track);
                    track.stop();
                });
                isRecording = false;
                audioButton.textContent = '🎤 Start Recording';
                audioButton.style.background = '#4CAF50';
                audioStatus.textContent = 'Ready to record';
                console.log('Recording stop requested');
            }
        }

        function stopRecording() {
            const audioButton = document.getElementById('audioButton');
            const audioStatus = document.getElementById('recordingStatus');

            console.log('stopRecording called');

            if (isRecording) {
                // Stop speech recognition
                if (recognition) {
                    console.log('Stopping speech recognition...');
                    recognition.stop();
                    recognition = null;
                }

                if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                    mediaRecorder.stop();
                }

                if (mediaRecorder && mediaRecorder.stream) {
                    mediaRecorder.stream.getTracks().forEach(track => {
                        console.log('Stopping track:', track);
                        track.stop();
                    });
                }

                isRecording = false;
                audioButton.textContent = '🎤 Start Recording';
                audioButton.style.background = '#4CAF50';
                audioStatus.textContent = 'Ready to record';
                console.log('Recording stopped');
            }
        }
    </script>
</body>
</html>